<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Emjay&#39;s DailyBlog</title>
  
  <subtitle>Emjay&#39;s DataScience &amp; Development</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://emjayahn.github.io/"/>
  <updated>2019-07-15T16:55:49.454Z</updated>
  <id>https://emjayahn.github.io/</id>
  
  <author>
    <name>EmjayAhn(Minjae Ahn)</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>iterator-generator</title>
    <link href="https://emjayahn.github.io/2019/07/15/iterator-generator/"/>
    <id>https://emjayahn.github.io/2019/07/15/iterator-generator/</id>
    <published>2019-07-15T14:03:23.000Z</published>
    <updated>2019-07-15T16:55:49.454Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Iterable-vs-Iterator-vs-Generator"><a href="#Iterable-vs-Iterator-vs-Generator" class="headerlink" title="Iterable vs Iterator vs Generator"></a>Iterable vs Iterator vs Generator</h1><hr><p>다른 분들의 코드를 읽을 때마다, 내가 사용할 때마다, 헷갈리는 Iterable, Iterator, Generator를 이번 글을 작성해보면서, 마지막으로! (라는 다짐으로) 정리해봅니다. 잘 알고 있는 개념이라고 생각했지만, 다른 사람들로부터의 질문을 받았을 때, 나의 설명이 만족스럽지 못해 ‘아 내가 더 정확히 알아야 한다’ 는 메타인지로부터 출발하는 글입니다.</p><a id="more"></a><p><br><br>파이썬의 장점으로 꼽히는, ‘사용하기 쉬운 데이터 구조’들 덕분에 우리는 루프를 돌아야하는 알고리즘에 대해 손쉽게 코드를 작성 할 수 있습니다. 하지만, 때로는 나만의 객체(class)를 만들고 그 객체가 파이썬에 내장 되어있는 데이터 구조 처럼, 동작하기를 바랍니다.<br><br><br>요즘 들어, 제가 짜는 코드에서 이 욕구는 신경망 모델링을 할 때, 신경망 모델 객체에 데이터의 배치를 feeding 하는 객체를 생성하고 싶을 때, 넘쳐나게 됩니다. 이럴 때, 파이썬에 대한 기본 개념이 잘 잡혀있지 않은 상태에서 복잡한 코드를 짜려고 하는 시도를 하니, 신경망 모델 자체의 구조에 대해서도 복잡한데 이런 루프를 돌면서 반복적으로 일정 데이터를 넘겨주기 위한 간단한 기능을 가진 코드에 대해서도 비효율적으로 작성하게 됩니다. 이런 제 자신의 문제를 해결하기 위해 이번 기회에 Iterator 와 Generator 에 대해 확실하게 정리해보려고 합니다. (feat. 예제코드)</p><hr><h2 id="1-이터러블-Iterable"><a href="#1-이터러블-Iterable" class="headerlink" title="1. 이터러블: Iterable"></a>1. 이터러블: Iterable</h2><blockquote><p>Iterable 객체란? : 객체 안에 있는 원소(element)를 하나씩 반환 가능한 객체</p></blockquote><p><img src="iterable_def.png" alt></p><p>파이썬이 제공하는 대부분의 내장 데이터 구조는 이터러블(Iterable)한 객체 입니다. 뿐만 아니라 우리가 만든 객체(class)도 Iterable 객체가 될 수 있습니다. 이터러블(Iterable)객체는 for 문과 같은 루프 뿐만 아니라, zip이나 map 과 같은 순서대로 처리할 입력이 필요한 곳에서도 사용 될 수 있습니다.<br><br><br><br>이터러블(Iterable) 객체는 <code>iter()</code> 라는 함수의 입력으로 들어갑니다. <code>iter()</code> 라는 함수는 다음에 설명될 이터레이터(Iterator)를 반환합니다.</p><hr><h2 id="2-이터레이터-Iterator"><a href="#2-이터레이터-Iterator" class="headerlink" title="2. 이터레이터: Iterator"></a>2. 이터레이터: Iterator</h2><blockquote><p>Iterator 객체란? : Iterator의 <code>__next__()</code> 나 내장 함수인 <code>next()</code>를 부르면서, 원소(element)를 순차적으로 반환 할 숫 있는 객체</p></blockquote><p><img src="iterator_def.png" alt></p><p>앞서, 설명드린대로 이터레이터(Iterator)는 iter()라는 함수가 <strong>반환</strong>하는 객체입니다. 그리고, 이터레이터(Iterator)는 반복적으로 <code>__next__()</code>나, <code>next()</code> 함수의 입력으로 들어가 호출하여, <code>next()</code>의 return 값인 원소(element)를 최종적으로 반환합니다.<br><br><br>이터레이터(Iterator)가 다음 원소를 계속 반환하다가, 끝에 다달아 반환할 원소가 없을 경우 예외문인 <code>StopIteration</code>이 발생하게 됩니다.<br><br><br>즉, 정리하면, <br><br>Iterable 객체 A → iter(A) → Iterator 객체 B → next(B) → element (data)</p><hr><blockquote><p>Question? 우리는 list 같은 이터러블(Iterable) 객체와 for 문을 쓰면서 한번도 iter() 나 next()를 보지 못했는뎁쇼????<br>파이썬의 for 문은 이터러블(Iterable) 객체를 만나, 내부적으로 <code>iter()</code> 함수를 호출하여, 이터레이터(Iterator)를 생성합니다. 이 생성된 이터레이터(Iterator)가 루프가 실행되면서 <code>next()</code> 를 호출하며 반복적인 데이터를 뽑아 낼 수 있게 되는 것입니다. 그리고 모든 원소가 뽑아지고 난 뒤에는 <code>StopIteration</code> 이 발생하며, for 문이 종료 됩니다.</p></blockquote><hr><p>위 설명을 코드로 풀어쓰면, 다음과 같습니다. 우리가 다음과 같은 for 문을 사용하면,</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">for</span> element <span class="hljs-keyword">in</span> iterable_object:</span><br><span class="line">    print(element)</span><br></pre></td></tr></table></figure><p>위 코드는 다음과 같이 파이썬 내부적으로 다음과 같이 동작하게 됩니다.</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># iter() 함수를 호출해, iterator 를 생성하고,</span></span><br><span class="line">iterator_object = iter(iterable_object)</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">while</span> <span class="hljs-keyword">True</span>:</span><br><span class="line">    <span class="hljs-comment"># next() 함수를 호출해, element 를 받아옵니다.</span></span><br><span class="line">    <span class="hljs-keyword">try</span>:</span><br><span class="line">        element = next(iterator_object)</span><br><span class="line">        print(element)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># element 가 없을 시, StopIteration Exception 발생</span></span><br><span class="line">    <span class="hljs-keyword">except</span>: StopIteration:</span><br><span class="line">        <span class="hljs-keyword">break</span></span><br></pre></td></tr></table></figure><hr><h2 id="3-이터러블-Iterable-이터레이터-Iterator-와-친해지기"><a href="#3-이터러블-Iterable-이터레이터-Iterator-와-친해지기" class="headerlink" title="3. 이터러블(Iterable), 이터레이터(Iterator)와 친해지기"></a>3. 이터러블(Iterable), 이터레이터(Iterator)와 친해지기</h2><h3 id="3-1-간단-버전"><a href="#3-1-간단-버전" class="headerlink" title="3-1. 간단 버전"></a>3-1. 간단 버전</h3><p>파이썬 이터러블(Iterable) 내장 데이터 구조인 list 를 활용하여, 실습해 봅니다.<br><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-meta">&gt;&gt;&gt; </span>iterable_object = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>]</span><br><span class="line"><span class="hljs-meta">&gt;&gt;&gt; </span>iterator_object = iter(iterable_object)</span><br><span class="line"><span class="hljs-meta">&gt;&gt;&gt; </span>iterator_object</span><br><span class="line">&lt;list_iterator object at <span class="hljs-number">0x104fe2278</span>&gt;</span><br><span class="line"><span class="hljs-meta">&gt;&gt;&gt; </span>next(iterator_object)</span><br><span class="line"><span class="hljs-number">1</span></span><br><span class="line"><span class="hljs-meta">&gt;&gt;&gt; </span>next(iterator_object)</span><br><span class="line"><span class="hljs-number">2</span></span><br><span class="line"><span class="hljs-meta">&gt;&gt;&gt; </span>next(iterator_object)</span><br><span class="line"><span class="hljs-number">3</span></span><br><span class="line"><span class="hljs-meta">&gt;&gt;&gt; </span>next(iterator_object)</span><br><span class="line"><span class="hljs-number">4</span></span><br><span class="line"><span class="hljs-meta">&gt;&gt;&gt; </span>next(iterator_object)</span><br><span class="line"><span class="hljs-number">5</span></span><br><span class="line"><span class="hljs-meta">&gt;&gt;&gt; </span>next(iterator_object)</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="hljs-string">"&lt;stdin&gt;"</span>, line <span class="hljs-number">1</span>, <span class="hljs-keyword">in</span> &lt;module&gt;</span><br><span class="line">StopIteration</span><br></pre></td></tr></table></figure></p><h3 id="3-2-나만의-iterable-iterator객체"><a href="#3-2-나만의-iterable-iterator객체" class="headerlink" title="3-2. 나만의 iterable, iterator객체"></a>3-2. 나만의 iterable, iterator객체</h3><p>다음의 코드는 이름과 나이를 받는 데이터 객체입니다. 루프를 돌면서 각각 순서에 맞는 (이름, 나이)형태로 데이터를 반환하는 Iterable 객체입니다. 같은 기능을 하는 더욱 효율적인 코드를 짤 수 있겠지만, 공부한 iterable 과 iterator 를 적용해보는 class 입니다.</p><script src="https://gist.github.com/EmjayAhn/70e4232632f340a8f67e0ba1509029c5.js"></script><hr><h2 id="4-제너레이터-Generator"><a href="#4-제너레이터-Generator" class="headerlink" title="4. 제너레이터: Generator"></a>4. 제너레이터: Generator</h2><blockquote><p>제너레이터(Generator)란?: <strong>특이한</strong> (공식 문서에 ~~~ iterator) 이터레이터(Iterator)</p></blockquote><p><img src="generator_def.png" alt></p><p>정의에서도 보다시피, 이터레이터(Iterator)입니다. 즉, <code>next()</code> 함수를 만나 동작하는 함수입니다. 이 때, 제너레이터(Generator) 를 일반 함수와 다르게 하는 것이 <code>yield</code> 라는 문법입니다. <code>yield</code> 는 일반 함수의 <code>return</code>과 같이 값을 반환 하지만, <code>return</code>과 다르게 해당 함수(generator)가 종료하지 않고, 그대로 유지됩니다. 다음 순서의 제너레이터(Generator)가 호출되면, 멈추었던 <code>yield</code> 자리에서 다시 함수가 동작하게 됩니다.<br><br></p><p>요약하자면, <br><br>제너레이터(Generator) → next(제너레이터) → 제너레이터 함수 실행 → <code>yield</code>를 만나 next(제너레이터가) 호출된 곳으로 값을 반환 (제너레이터 종료 ❌) → 다시 next(제너레이터) → 멈추었던 <code>yield</code>부터 재실행 </p><hr><blockquote><p>Question? 이런 특이하고, 어렵고, 처음엔 익숙하지 않은 제너레이터(Generator)를 왜 때문에 쓰는 겁죠???<br>제너레이터는 메모리를 아끼기 위해서 사용합니다!!! 다음의 코드에서 메모리 사용량의 비교를 통해 살펴 보겠습니다.</p></blockquote><hr><h2 id="5-제너레이터-Generator-와-친해지기"><a href="#5-제너레이터-Generator-와-친해지기" class="headerlink" title="5. 제너레이터 (Generator)와 친해지기"></a>5. 제너레이터 (Generator)와 친해지기</h2><p>제너레이터를 사용한 코드가 메모리 사용량 측면에서 얼마나 효율적인지 확인해보겠습니다. 다음의 코드는 999999까지의 숫자를 제곱하여 return 해주는 함수입니다. 첫번째, 일반적인 함수는 end 숫자까지 for 문을 돌면서 그 결과를 저장한 뒤에 return 해줍니다. 두번째 generator 는 for 문이 돌 때, yield 에서 해당 데이터만 리턴하게 되므로, 메모리 사용에서 효율적입니다. 다음의 실험에서도 쉽게 비교할 수 있습니다.</p><script src="https://gist.github.com/EmjayAhn/943cde0f7e9794f76272e531c8fffcca.js"></script>위 코드 실행결과: <br><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Memory Usage when program start: 8.80859375</span><br><span class="line">Memory Usage when program end: 56.0390625</span><br></pre></td></tr></table></figure><script src="https://gist.github.com/EmjayAhn/8ab50ebfdbf3adabeab8f07f20dd0e01.js"></script><p>위 코드 실행결과: <br><br><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Memory Usage when program start: 8.78515625</span><br><span class="line">Memory Usage when program end: 8.78515625</span><br></pre></td></tr></table></figure></p><hr><h2 id="6-마무리"><a href="#6-마무리" class="headerlink" title="6. 마무리"></a>6. 마무리</h2><p>이번 짧은 글에서, Iterable, Iterator, Generator 를 비교해보면서 그 개념과 사용 예제를 간단하게 살펴 보았습니다. 이름에서부터 헷갈릴 수 있는 각 객체에 대한 내용을 이 글을 통해 조금이나마 정리해볼 수 있는 기회가 되셨으면 좋겠으며, 작은 도움이 되셨으면 합니다.</p><hr><h2 id="7-Reference"><a href="#7-Reference" class="headerlink" title="7. Reference"></a>7. Reference</h2><ul><li><a href="https://docs.python.org/3.7/glossary.html#term-generator" target="_blank" rel="noopener">Python documentation: generator</a></li><li><a href="https://docs.python.org/3.7/glossary.html#term-iterable" target="_blank" rel="noopener">Python documentation: iterable</a></li><li><a href="https://docs.python.org/3.7/glossary.html#term-iterator" target="_blank" rel="noopener">Python documentation: iterator</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Iterable-vs-Iterator-vs-Generator&quot;&gt;&lt;a href=&quot;#Iterable-vs-Iterator-vs-Generator&quot; class=&quot;headerlink&quot; title=&quot;Iterable vs Iterator vs Generator&quot;&gt;&lt;/a&gt;Iterable vs Iterator vs Generator&lt;/h1&gt;&lt;hr&gt;
&lt;p&gt;다른 분들의 코드를 읽을 때마다, 내가 사용할 때마다, 헷갈리는 Iterable, Iterator, Generator를 이번 글을 작성해보면서, 마지막으로! (라는 다짐으로) 정리해봅니다. 잘 알고 있는 개념이라고 생각했지만, 다른 사람들로부터의 질문을 받았을 때, 나의 설명이 만족스럽지 못해 ‘아 내가 더 정확히 알아야 한다’ 는 메타인지로부터 출발하는 글입니다.&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="https://emjayahn.github.io/categories/Python/"/>
    
    
  </entry>
  
  <entry>
    <title>CS224n-Lecture01-Summary</title>
    <link href="https://emjayahn.github.io/2019/07/06/CS224n-Lecture01-Summary/"/>
    <id>https://emjayahn.github.io/2019/07/06/CS224n-Lecture01-Summary/</id>
    <published>2019-07-06T08:38:25.000Z</published>
    <updated>2019-07-15T16:56:37.494Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CS224n-Lecture-1-Introduction-and-Word-Vectors"><a href="#CS224n-Lecture-1-Introduction-and-Word-Vectors" class="headerlink" title="[CS224n] Lecture 1: Introduction and Word Vectors"></a>[CS224n] Lecture 1: Introduction and Word Vectors</h1><p>Standford University 의 CS224n 강의를 듣고 정리하는 글입니다.</p><a id="more"></a><h2 id="1-Human-Language-and-Word-Meaning"><a href="#1-Human-Language-and-Word-Meaning" class="headerlink" title="1. Human Language and Word Meaning"></a>1. Human Language and Word Meaning</h2><p><strong>Word meaning</strong> 의 뜻 : symbol → idea or thing : Denotational Semantics</p><p>우리가 ‘의자’라는 단어를 예로 들자면, 말하는 사람과 듣는 사람 모두 의자와 관련된 특정한 이미지와 아이디어 등을 생각해 볼 수 있다. 이렇게 단어에 대해 관념적인 것을 word meaning 이라고 할 수 있다. 그렇다면 컴퓨터에서 사용할 수 있는 <strong>word meaning</strong> 은 어떤 것이 있을까?</p><h3 id="In-Computer-Word-Representations"><a href="#In-Computer-Word-Representations" class="headerlink" title="In Computer Word Representations"></a>In Computer Word Representations</h3><ol><li>WordNet: synonym 과 hypernyms 의 집합으로 표현 (e.g: nltk wordnet)<ul><li>아주 귀한 dataset 이지만,</li><li>단점: Nuance (뉘앙스)를 담아내지 못함, 새로운 단어에 대해 사람이 직접 가공하여 추가해주어야 한다. 단어의 동의어에 대해 계산할 수 없음</li></ul></li><li>Discrete Symbols: One-hot-vectors<ul><li>단점 : 벡터의 차원이 우리가 가지고 있는 단어의 갯수만큼 커지게 된다. Corpus 의 구성 단어가 커질 수록 계산 해야하는 벡터 차원이 매우 커진다. 이는 컴퓨팅 자원의 부족으로 인한 현실적 구현의 어려움을 야기시킨다.</li><li>One-hot-Vector 는 Vector Space 에서 모두 서로 orthogonal (직교)한다. 즉, similarity 가 모두 0으로 단어간의 관계를 알기 힘들다.</li></ul></li><li>By CONTEXT: Word Vecttors<ul><li>“You shall know a word by the company it keeps” (J.R.Firth)</li><li>어떤 특정 단어 w 가 나온다는 것은, 그 주변 단어들(context)이  있기 때문이다.</li><li>Word Vectors == Word Embeddings == Word Representations == Distributed Representation</li></ul></li></ol><h2 id="2-Word2vec-Introduction"><a href="#2-Word2vec-Introduction" class="headerlink" title="2. Word2vec Introduction"></a>2. Word2vec Introduction</h2><p>Word2vec(Mikolov et al. 2013) 은 Word Vectors 를 만들기 위한 알고리즘 중 가장 기초 뼈대를 이루는 알고리즘이다.</p><ol><li>가지고 있는 텍스트 데이터를 구성하는 큰 CORPUS</li><li>Corpus 의 모든 단어는 RandomVector 로 시작한다.</li><li>각 position t 에 대해, 중심단어 c(center) 와 주변단어 o(outside) 를 생각할 수 있다.</li><li>중심단어 c 가 주어질 때, 주변단어 o 의 확률을 구하기 위해(skip-gram)(반대로도 가능(cbow): 주변단어가 주어질 때, 중심단어의 확률을 구하는 방법), 중심단어 c 와 주변단어 o, word 벡터들에 대해 similarity를 구한다.</li><li>위의 확률을 최대화 하기 위해 word vector 를 updating 한다.</li></ol><p><img src="Untitled-cc28ca48-12e4-4a20-a6c4-b0be86d339d8.png" alt></p><h2 id="3-Word2vec-objective-function-gradients"><a href="#3-Word2vec-objective-function-gradients" class="headerlink" title="3. Word2vec objective function gradients"></a>3. Word2vec objective function gradients</h2><p>문장의 특정 위치 t 에 대해 (t = 1, …, T), 중심단어w_j가 주어졌을때, 주변단어를 한정하는 window size m 에 대해  주변단어들을 예측하는 Likelihood 는 다음과 같습니다. likelihood 식을 해석해보면, 중심단어 w_t 에 대해 중심단어를 중심으로 window 사이즈 2m 개의 단어들의 확률을 모두 곱하고, T 개의 단어 갯수에 대해 또 다시 모두 곱합니다.</p><script type="math/tex; mode=display">Likelihood\quad L(\theta)= \prod_{t=1}^{T}\prod_{-m \leq j \leq m}P(w_{t+j}|w_{t};\theta)</script><p>목적함수는 likelihood 를 바탕으로 minimize와 average, 계산 편의를 위해 log 를 씌웠다는 것외에 likelihood 와 동일하다.</p><script type="math/tex; mode=display">objective function \quad J(\theta) = - \frac{1}{T}logL(\theta)=-\frac{1}{T}\sum_{t=1}^{T}\sum_{-m \leq j \leq m, j\neq0}logP(w_{t+j} | w_{t};\theta)</script><p>단어가 등장할 확률을 구하는 방법은 Softmax Function 을 활용합니다.</p><p><img src="Untitled-99acc04b-91fd-451c-868f-574d995778b3.png" alt></p><h3 id="Objective-Function-의-derivative"><a href="#Objective-Function-의-derivative" class="headerlink" title="Objective Function 의 derivative"></a>Objective Function 의 derivative</h3><p>Objective Function 과 단어 확률 (Softmax) 의 미분을 구하는 과정이다. </p><p><img src="Untitled-d45516b7-8024-457c-bda2-39092a1f6892.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;CS224n-Lecture-1-Introduction-and-Word-Vectors&quot;&gt;&lt;a href=&quot;#CS224n-Lecture-1-Introduction-and-Word-Vectors&quot; class=&quot;headerlink&quot; title=&quot;[CS224n] Lecture 1: Introduction and Word Vectors&quot;&gt;&lt;/a&gt;[CS224n] Lecture 1: Introduction and Word Vectors&lt;/h1&gt;&lt;p&gt;Standford University 의 CS224n 강의를 듣고 정리하는 글입니다.&lt;/p&gt;
    
    </summary>
    
      <category term="Lecture" scheme="https://emjayahn.github.io/categories/Lecture/"/>
    
      <category term="CS224n" scheme="https://emjayahn.github.io/categories/Lecture/CS224n/"/>
    
    
      <category term="CS224n" scheme="https://emjayahn.github.io/tags/CS224n/"/>
    
      <category term="summary" scheme="https://emjayahn.github.io/tags/summary/"/>
    
  </entry>
  
  <entry>
    <title>[GCP] Computing Engine 환경설정</title>
    <link href="https://emjayahn.github.io/2019/06/17/gcp-setting/"/>
    <id>https://emjayahn.github.io/2019/06/17/gcp-setting/</id>
    <published>2019-06-17T11:54:01.000Z</published>
    <updated>2019-07-15T16:56:59.085Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Computing-Engine-환경설정"><a href="#Computing-Engine-환경설정" class="headerlink" title="Computing Engine 환경설정"></a>Computing Engine 환경설정</h1><ul><li>이번 글은, Google Cloud Platform의 Computing Engine 인스턴스의 기본적인 환경설정 방법을 소개하는 글입니다. 직접 작업환경을 세팅하면서, 정리 하는 목적으로 작성하는 글입니다.<br></li></ul><a id="more"></a><ul><li>다음과 같은 환경을 설정합니다.</li></ul><ol><li>인스턴스 생성 및 네트워크 설정</li><li>접속을 위한 ssh 생성 및 접속</li><li>python3, pip3 설치</li><li>CUDA 설치</li><li>cuDNN 설치</li><li>Pytorch 설치</li><li>Jupyter 설치 및 환경설정</li></ol><h2 id="1-인스턴스-생성-및-네트워크-설정"><a href="#1-인스턴스-생성-및-네트워크-설정" class="headerlink" title="1. 인스턴스 생성 및 네트워크 설정"></a>1. 인스턴스 생성 및 네트워크 설정</h2><p>Compute Engine에서 자신의 목적에 맞는 리소스를 정해, 인스턴스를 생성해줍니다.</p><p><img src="Untitled-aa8ea35c-8d35-4efb-8fda-520336241429.png" alt></p><p>Jupyter  notebook 을 사용하기 위해, VPC 네트워크 → 방화벽 규칙 탭에서 방화벽 규칙을 만들어 줍니다. 후에 Tensorboard 와 다른 기타 환경들을 사용할 때 그에 맞는 포트 규칙을 동일한 방법으로 열어주면 됩니다. 아래의 4가지를 설정해주고 ‘만들기’ 클릭</p><p>이름 : jupyter</p><p>소스 IP 범위: 0.0.0.0/0</p><p>대상 태그: jupyter</p><p>tcp: 8888</p><p>http, https: 체크</p><p><img src="Untitled-e502c0fe-c454-4da1-bf99-5ee9c057bab5.png" alt></p><h2 id="2-접속을-위한-RSA-key-pair-생성-및-ssh-접속"><a href="#2-접속을-위한-RSA-key-pair-생성-및-ssh-접속" class="headerlink" title="2. 접속을 위한 RSA key pair 생성 및 ssh 접속"></a>2. 접속을 위한 RSA key pair 생성 및 ssh 접속</h2><ul><li>gcp 에서 제공하는 gcloud로도 접속 할 수 있습니다.</li></ul><ol><li><p>위에서 생성한 인스턴스에 접속하기 위해, RSA key pair 를 이를 통해 접속 해 봅니다. 아래의 명령어를 이용해 키페어를 생성해 줍니다. 이 때, USERNAME 은 gcp에 등록한 이메일로 설정합니다.</p><p> $ ssh-keygen -t rsa -f ~/.ssh/[KEYFILE_NAME] -C “[USERNAME]”</p><p> example) $ ssh-keygen -t rsa -f ~/.ssh/gcp-key -C “myemail@mail.com”</p></li></ol><p>앞으로 사용할 Password 를 입력하고, </p><p>생성된 키페어는 <code>.ssh</code> 폴더 안에서 확인 할 수 있습니다. <code>.ssh/[KEYFILE_NAME].pub</code> 를 확인해 볼 수 있습니다.</p><ol><li>생성한 키페어를 gcp 의 메타데이터 탭 → SSH 키에 등록합니다.</li></ol><p><img src="Untitled-ac2cd300-7176-46cc-92d2-aa405a955f85.png" alt></p><ol><li><p>ssh 접속</p><p> $ ssh -i ~/.ssh/[KEYFILE_NAME] [USERNAME]@[GCP외부IP]</p></li></ol><h2 id="3-Python3-PIP-설치"><a href="#3-Python3-PIP-설치" class="headerlink" title="3. Python3, PIP 설치"></a>3. Python3, PIP 설치</h2><p>위에서 서버에 접속했다면, 서버의 개발환경을 설정해 주기만 하면 됩니다. python3 와 pip 부터 설치해 봅니다.</p><ol><li>locale 설정<figure class="highlight bash hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt install language-pack-ko</span><br><span class="line">$ sudo locale-gen ko_KR.UTF-8</span><br><span class="line"></span><br><span class="line">$ <span class="hljs-built_in">export</span> LC_ALL=<span class="hljs-string">"en_US.UTF-8"</span></span><br><span class="line">$ <span class="hljs-built_in">export</span> LC_CTYPE=<span class="hljs-string">"en_US.UTF-8"</span></span><br><span class="line">$ sudo dpkg-reconfigure locales</span><br></pre></td></tr></table></figure></li></ol><p>en_US.UTF-8이 [*] 로 체크 되어 있는지까지 확인합니다.</p><ol><li>Python3, PIP 설치<figure class="highlight bash hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt update</span><br><span class="line">$ sudo apt install python3-pip</span><br></pre></td></tr></table></figure></li></ol><p>설치 후, <code>python3 --version</code> 으로 python 이 잘 설치 되었는지 확인합니다.</p><h2 id="4-CUDA-설치"><a href="#4-CUDA-설치" class="headerlink" title="4. CUDA 설치"></a>4. CUDA 설치</h2><p>우리가 가장 원하는 리소스인 gpu를 활용한 연산을 위해 CUDA 를 설치 해 줍니다.</p><ol><li>먼저, 설치 파일을 다운로드 해줍니다.</li></ol><figure class="highlight bash hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 루트에서</span><br><span class="line">$ wget [http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/](https://developer.nvidia.com/compute/cuda/10.0/Prod/local_installers/cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64)</span><br></pre></td></tr></table></figure><ol><li><code>ls</code> 로 cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64.deb 이 잘 다운로드 되었는지 확인 해 줍니다. 다운로드 결과 <code>.deb</code> 확장자가 되어있지 않다면, 파일명에 .deb 를 뒤에 붙여 줍니다.</li></ol><p><code>mv cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64 cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64.deb</code></p><ol><li>설치<figure class="highlight bash hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo dpkg -i cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64.deb</span><br><span class="line">$ sudo apt add /var/cuda-repo-&lt;version&gt;/7fa2af80.pub</span><br><span class="line">$ sudo apt update</span><br><span class="line">$ sudo apt install cuda</span><br></pre></td></tr></table></figure></li></ol><p>CUDA 가 정상적으로 설치되었다면,  <code>$ nvidia-smi</code> 를 통해 자신의 gpu 상태를 확인 할 수 있습니다. 또한, <code>/usr/local/cuda/version.txt</code> 에 설치한 CUDA, 현재는 10.0의 version 을 확인 할 수 있습니다.</p><h2 id="5-cuDNN-설치"><a href="#5-cuDNN-설치" class="headerlink" title="5. cuDNN 설치"></a>5. cuDNN 설치</h2><p>다음의 명령어를 통해 cuDNN 을 설치 할 수 있습니다.<br><figure class="highlight bash hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo sh -c <span class="hljs-string">'echo "deb http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 /" &gt;&gt; /etc/apt/sources.list.d/cuda.list'</span></span><br><span class="line">$ sudo apt update</span><br><span class="line">$ sudo apt install libcudnn7-dev</span><br></pre></td></tr></table></figure></p><h2 id="6-Pytorch-설치"><a href="#6-Pytorch-설치" class="headerlink" title="6. Pytorch 설치"></a>6. Pytorch 설치</h2><ul><li>우리는 서버가 https 프로토콜을 사용한다고 체크했으므로 -H  flag 를 주어 sudo pip3 를 활용해야합니다.<figure class="highlight bash hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo pip3 -H install https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl</span><br><span class="line">$ sudo pip3 -H install https://download.pytorch.org/whl/cu100/torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl</span><br></pre></td></tr></table></figure></li></ul><h2 id="7-Jupyter-Notebook-설치-및-환경설정"><a href="#7-Jupyter-Notebook-설치-및-환경설정" class="headerlink" title="7. Jupyter Notebook 설치 및 환경설정"></a>7. Jupyter Notebook 설치 및 환경설정</h2><ol><li>Jupyter를 설치합니다.<figure class="highlight bash hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo -H pip3 install jupyter</span><br></pre></td></tr></table></figure></li></ol><p>설치 후, <code>$ jupyter notebook</code> 으로 주피터 커널이 켜지는지 확인합니다.</p><ol><li><p>주피터 config 파일을 생성합니다.</p><figure class="highlight bash hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ jupyter notebook --generate-config</span><br></pre></td></tr></table></figure></li><li><p>비밀번호를 생성합니다. 이 비밀번호는 지금 설치한 주피터 환경에 들어가기 위한 비밀번호 입니다.</p><figure class="highlight bash hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ ipython</span><br><span class="line">from notebook.auth import passwd</span><br><span class="line">passwd()</span><br><span class="line">Enter Password: 사용할 비밀번호 입력</span><br><span class="line">Verify Password: 사용할 비밀번호 입력</span><br></pre></td></tr></table></figure></li></ol><p>출력된 비밀번호 해쉬  <code>sha1: ~~~</code> 를 복사해 둡니다.</p><ol><li>우리의 인스턴스는 https 프로토콜을 사용하므로, SSL 키파일을 생성해야 합니다.<figure class="highlight bash hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout cert.pem -out cert.pem</span><br></pre></td></tr></table></figure></li></ol><p>위 명령어를 입력하고, 뒤따라 나오는 정보들을 입력하여, <code>.pem</code> 파일을 생성합니다.</p><ol><li><ol><li>에서 생성한 config 파일 수정<figure class="highlight bash hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vi /home/[USERNAME]/.jupyter/jupyter_notebook_config.py</span><br></pre></td></tr></table></figure></li></ol></li></ol><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// config 파일에 다음의 내용을 추가합니다.</span><br><span class="line">c = get_config()</span><br><span class="line">c.NotebookApp.ip = &apos;내부아이피주소&apos;</span><br><span class="line">c.NotebookApp.open_browser=False</span><br><span class="line">c.NotebookApp.password=&apos;3에서 생성한 비밀번호 해쉬 sha1:~&apos;</span><br><span class="line">c.Notebook.certfile=&apos;4에서 생성한 .pem 파일 경로 /home/USERNAME/cert.pem&apos;</span><br></pre></td></tr></table></figure><p>위까지 완료하게 되었으면, 주피터 서버를 열고,  <code>https://외부아이피:8888</code>  로 주피터를 접속 하는 것을 확인하시면 되겠습니다.</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Computing-Engine-환경설정&quot;&gt;&lt;a href=&quot;#Computing-Engine-환경설정&quot; class=&quot;headerlink&quot; title=&quot;Computing Engine 환경설정&quot;&gt;&lt;/a&gt;Computing Engine 환경설정&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;이번 글은, Google Cloud Platform의 Computing Engine 인스턴스의 기본적인 환경설정 방법을 소개하는 글입니다. 직접 작업환경을 세팅하면서, 정리 하는 목적으로 작성하는 글입니다.&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Google Cloud Platform" scheme="https://emjayahn.github.io/categories/Google-Cloud-Platform/"/>
    
    
      <category term="Google Cloud Platform" scheme="https://emjayahn.github.io/tags/Google-Cloud-Platform/"/>
    
  </entry>
  
  <entry>
    <title>Classification Metrics</title>
    <link href="https://emjayahn.github.io/2019/06/03/Classification-Metrics/"/>
    <id>https://emjayahn.github.io/2019/06/03/Classification-Metrics/</id>
    <published>2019-06-03T03:18:57.000Z</published>
    <updated>2019-07-15T16:57:48.397Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Classification-Metrics-분류-성능-지표"><a href="#Classification-Metrics-분류-성능-지표" class="headerlink" title="Classification Metrics: 분류 성능 지표"></a>Classification Metrics: 분류 성능 지표</h1><p>Kaggle Classification 의 Evaluation 에 자주 사용되는 ROC-AUC를 정리해보면서, 이번 기회에 분류모델에서 자주 사용되는 성능지표(Metric)을 간단하게 정리해봅니다.<br><a id="more"></a><br>Confusion Matrix 에서 비롯되는 Metric 들은 이를 이미지로 기억하는 것이 효율적입니다.</p><h2 id="Confusion-Matrix"><a href="#Confusion-Matrix" class="headerlink" title="Confusion Matrix"></a>Confusion Matrix</h2><p>confusion matrix  는 해당 데이터의 정답 클래스(y_true) 와 모델의 예측 클래스(y_pred)의 일치 여부를 갯수로 센 표입니다. 주로, 정답 클래스는 행으로(row), 예측 클래스는 열로(columns) 표현합니다. </p><p><img src="Untitled-170ac3e3-8259-4e1a-80e5-647b5bd326fd.png" alt></p><ul><li>(정의하기에 따라 다르지만) 일반적으로, class 1 을 positive로, class 0 을 negative 로 표기합니다.</li><li>우리가 예측한 클래스를 기준으로 Positive 와 Negative 를 구분합니다.</li><li>그리고 정답과 맞았는지, 틀렸는지를 알려주기 위해 True 와 False 를 각각 붙여줍니다.</li></ul><h2 id="Accuracy-정확도"><a href="#Accuracy-정확도" class="headerlink" title="Accuracy: 정확도"></a>Accuracy: 정확도</h2><ul><li>전체 데이터에 대해 맞게 예측한 비율</li></ul><script type="math/tex; mode=display">\frac{TP+TN}{TP+FN+FP+TN}</script><h2 id="Precision-정밀도"><a href="#Precision-정밀도" class="headerlink" title="Precision: 정밀도"></a>Precision: 정밀도</h2><ul><li>class 1 이라고 예측한 데이터 중, 실제로 class 1 인 데이터의 비율</li></ul><script type="math/tex; mode=display">\frac{TP}{TP+FP}</script><p>우리의 모델이 기본적인 decision이 class 0 이라고 생각할 때, class 1 은 특별한 경우를 detect 한 경우 일 것입니다. 이 때 class 1 이라고 알람을 울리는 우리의 모델이 얼마나 세밀하게 class를 구분 할 수 있는지의 정도를 수치화 한 것입니다.</p><h2 id="Recall-재현율"><a href="#Recall-재현율" class="headerlink" title="Recall: 재현율"></a>Recall: 재현율</h2><ul><li>실제로 class 1 인 데이터 중에 class 1 이라고 예측한 비율</li><li>= Sensivity = TPR</li></ul><script type="math/tex; mode=display">\frac{TP}{TP+FN}</script><p>제가 기억하는 방식은, 자동차에 결함이 발견되서 recall 이 되어야 하는데 (실제 고장 데이터중) 얼마나 recall 됐는지로 생각합니다. </p><h2 id="Fall-out-위양성율"><a href="#Fall-out-위양성율" class="headerlink" title="Fall-out: 위양성율"></a>Fall-out: 위양성율</h2><ul><li>실제로 class 1 이 아닌 데이터 중에 class 1이라고 예측한 비율</li><li>낮을 수록 좋음</li><li>= FPR = 1 - Specificity</li><li>Specificity = 1 - Fall out</li></ul><script type="math/tex; mode=display">\frac{FP}{FP+TN}</script><p>실제로 양성데이터가 아닌 데이터에 대해서 우리의 모델이 양성이라고 잘못 예측한 비율을 말합니다. 억울한 데이터의 정도를 측정했다고 생각 할 수 있습니다.</p><h2 id="각-Metric-간의-상관관계"><a href="#각-Metric-간의-상관관계" class="headerlink" title="각 Metric 간의 상관관계"></a>각 Metric 간의 상관관계</h2><p>우리 모델의 decision function 을 f(x) 라 할 때, 우리는 f(x)의 결과와 threshold (decision point)를 기준으로 class를 구분합니다.</p><ul><li><p>Recall vs Fall-out : 양의 상관관계</p><p>  Recall은 위의 정의에 의하듯이, 실제로 positive  인  클래스에 대해 얼마나 positve 라고 예측했는지의 비율입니다. 우리가 Recall 을 높이기 위해서는 고정되어있는 실제 positive 데이터 수에 대해 예측하는 positive 데이터의 갯수 threshold 를 낮춰 늘리면 됩니다. 이에 반해 threshold 를 낮추게 되면, 실제로 positive 가 아닌 데이터에 대해 positive 라고 예측하는 억울한 데이터가 많아지므로 Fall-out 은 커지게 되고 둘은 양의 상관관계를 갖게 됩니다.</p></li><li><p>Recall vs Precision : 대략적인 음의 상관관계</p><p>  위에 설명한 것처럼 threshold 를 낮춰 우리가 예측하는 positive 클래스의 숫자를 늘리게 되면, recall 은 높아지는 반면, 예측한 positive 데이터 중 실제 positive 데이터의 비율은 작아 질 수 있습니다.</p></li></ul><h2 id="F-beta-score"><a href="#F-beta-score" class="headerlink" title="F-beta score"></a>F-beta score</h2><ul><li>precision 과 recall의 가중 조화평균</li></ul><script type="math/tex; mode=display">(\frac{1}{1+\beta^2}\frac{1}{precision} + \frac{\beta^2}{1+\beta^2}\frac{1}{recall})^{-1}</script><p>이처럼, 다양한 Metric 에 대해 우리가 초점을 맞추는 것에 따라 모델의 성능은 다르게 바라 볼 수 있습니다. 따라서 모델에 대해 성능을 평가하고 최종 모델을 선택함에 있어, 서로 다른 Metric 을 동시에 비교해야합니다. 이를 위해 precision 과 recall 을 precision 에 beta^2 만큼 가중하여 바라보는 스코어가 F beta score 입니다.</p><p>이 중 beta=1 일 때, score 가 우리가 자주 보는 f1 score 입니다.</p><script type="math/tex; mode=display">F_1=\frac{2*precision*recall}{precision+recall}</script><h2 id="ROC-Curve-Receiver-Operator-Characteristic-Curve"><a href="#ROC-Curve-Receiver-Operator-Characteristic-Curve" class="headerlink" title="ROC Curve: Receiver Operator Characteristic Curve"></a>ROC Curve: Receiver Operator Characteristic Curve</h2><ul><li>Recall vs Fallout 의 plot (TPR vs FPR)</li></ul><p>위의 예시 처럼, 우리가 클래스를 판별하는 기준이 되는 threshold (decision point) 를 올리거나 내리면서, recall 과 fall out 은 바뀌게 됩니다. 이렇게 threshhold 를 변화 해 가면서, recall 과 fall out 을 plotting 한 것이 ROC curve 입니다.</p><ul><li>sklearn.metrics.roc_curve() 의 documentation</li></ul><p><img src="Untitled-1f189e32-c453-43c9-8366-559df3f16464.png" alt></p><h2 id="ROC-AUC-Area-Under-Curve"><a href="#ROC-AUC-Area-Under-Curve" class="headerlink" title="(ROC-)AUC: Area Under Curve"></a>(ROC-)AUC: Area Under Curve</h2><ul><li>위에서 그린 ROC Curve 의 넓이를 점수로써 사용하는 것이 AUC 입니다. AUC 의 유의미한 범위는 class 를 50%의 확률로 random 하게 예측한 넓이인 0.5 보다는 클 것이고, 가장 최대의 AUC 의 넓이는 1 일 것이므로 0.5≤AUC≤1 의 범위를 갖는 score 입니다.</li></ul><p><img src="Untitled-44d5d317-d0ec-4f1a-8aa0-8bdbfe34bd67.png" alt></p><ul><li>ROC 커브와 AUC score 를 보고 모델에 대한 성능을 평가 하기 위해서, ROC 는 같은 Fall out 에 대해 Recall  은 더 높길 바라고, 같은 Recall  에 대해서는, Recall  이 더 작길 바랍니다. 결국, 그래프가 왼쪽 위로 그려지고, AUC 즉 curve  의 넓이는 커지는 것이 더 좋은 성능의 모델이라고 볼 수 있습니다.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Classification-Metrics-분류-성능-지표&quot;&gt;&lt;a href=&quot;#Classification-Metrics-분류-성능-지표&quot; class=&quot;headerlink&quot; title=&quot;Classification Metrics: 분류 성능 지표&quot;&gt;&lt;/a&gt;Classification Metrics: 분류 성능 지표&lt;/h1&gt;&lt;p&gt;Kaggle Classification 의 Evaluation 에 자주 사용되는 ROC-AUC를 정리해보면서, 이번 기회에 분류모델에서 자주 사용되는 성능지표(Metric)을 간단하게 정리해봅니다.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="MachineLearning" scheme="https://emjayahn.github.io/categories/MachineLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>SPARK BASIC 1</title>
    <link href="https://emjayahn.github.io/2019/05/31/SPARK-BASIC-1/"/>
    <id>https://emjayahn.github.io/2019/05/31/SPARK-BASIC-1/</id>
    <published>2019-05-31T12:07:52.000Z</published>
    <updated>2019-05-31T12:12:26.958Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Apache-Spark-Basic-1"><a href="#Apache-Spark-Basic-1" class="headerlink" title="Apache Spark Basic 1"></a>Apache Spark Basic 1</h1><ul><li>SPARK 를 공부하면서 실습 과정을 정리해서 남깁니다.</li><li>실습환경<ol><li>CentOS</li><li>Spark 2.4.3</li><li>Hadoop 2.7</li></ol></li></ul><h1 id="1-Spark-shell"><a href="#1-Spark-shell" class="headerlink" title="1. Spark-shell"></a>1. Spark-shell</h1><h2 id="1-1-Introduction"><a href="#1-1-Introduction" class="headerlink" title="1-1. Introduction"></a>1-1. Introduction</h2><p>shell의 spark home directory 에서 다음 명령어를 통해 spark shell 을 진입할 수 있습니다.</p><pre><code>$ cd /spark_home_directory/$ ./bin/spark-shell</code></pre><p><img src="Untitled-d3bd9451-d568-49aa-bf5a-0f6b7bf598d9.png" alt></p><ul><li>sc : spark context</li><li>spark : spark session</li></ul><p>spark context 와 spark session 의 경우, spark shell에 띄우면서 내부적으로 선언된 변수명이다. </p><pre><code>$ jps// jps 명령어를 통해 현재 돌고 있는 spark process 를 확인할 수 있다. // spark processor 가 jvm 을 바탕으로 돌기 때문에, jvm 프로세스가 도는 것을 확인하므로써// 확인 할 수 있는 것이다.</code></pre><p><a href="http://localhost:4040" target="_blank" rel="noopener">http://localhost:4040</a>, 즉 해당 서버의 ip:4040 포트를 통해서 드라이버에서 제공되는 웹 UI 를 확인할 수 있다. 이 웹 UI 를 통해 현재 작동하는 프로세서와 클러스터들을 관리 할 수 있다.</p><h2 id="1-2-RDD"><a href="#1-2-RDD" class="headerlink" title="1-2. RDD"></a>1-2. RDD</h2><p>spark는 data를 처리할 때, RDD 와 Spark SQL 을 통해서 data object 를 생성하고 이를 바탕으로 다양한 pipeline 으로 동작 할 수 있다. RDD 를 처음 접해보는 실습.</p><pre><code>//scala shellval data = 1 to 10000val distData = sc.parallelize(data)distData.filter(_ &lt; 10).collect()</code></pre><ul><li><code>data</code> 가 RDD</li><li>sc.parallelize의 return 형 역시 parallelize 된 RDD, 즉 distData 도 RDD</li><li>마지막 command line 은 10보다 작은 data 에 대해 filtering 하고 각 executor 에서 실행된 자료를 collect()</li><li>spark 의 특징은 .collect() 와 같은 action api 가 실행될 때 모든 것이 실행되는 <strong>Lazy Evaluation (RDD)</strong>으로 동작한다.</li></ul><p>드라이버 웹 UI 를 통해 이를 확인 할 수 있다. 이전 command line 에서는 아무 동작도 일어나지 않다가 collect() action api 수행을 통해 실제로 command들이 수행되는 것을 확인 할 수 있다. local 에서 default 로 동작하기 때문에 2개의 partition 으로 동작하며, 어떤 shuffling 도 일어나지 않았기 때문에 1개의 stage 임을 확인 할 수 있다.</p><p><img src="Untitled-a49c2d30-d7d2-43bf-a2d0-7a54ff94d0c6.png" alt></p><pre><code>// scala shell// sc.textFile 을 통해 textfile, md 파일등을 읽어드릴 수 있다.val data = sc.textFile(&quot;file_name&quot;)// rdd 의 .map api 를 통해서 rdd 의 element 마다 val distData = data.map(r =&gt; r + &quot;_experiment!!&quot;)// 앞선 map 이 수행되고, 각 element(data) 갯수를 세개 된다.distData.count</code></pre><p>여기서는 .count 가 action api 이므로, .count 가 수행될 때, 앞선 command 들이 수행되게 된다.</p><p><code>sc.textFile()</code> 의 경우 ‘\n’, newline 을 기준으로 element 를 RDD 에 담게 된다.</p><p><code>RDD.toDebugString</code> 를 통해 해당 RDD 의 Lineage 를 확인 할 수 있다.</p><ul><li>가장 왼쪽에 있는 | 를 통해 stage 정보 역시 확인 할 수 있다. shuffle 이 일어나게 되면, stage가 바뀌므로, 서로 다른 stage  에 있는 command 의 경우, 다른 indent에 있게 된다.</li></ul><p><img src="Untitled-62b02b6f-4150-4dcf-8543-655381c951f7.png" alt></p><p><code>RDD.getNumPartitions</code> 를 통해 해당 RDD의 Partition 갯수 (=Task 의 갯수), 즉 병렬화 수준을 확인 할 수 있다.</p><p><img src="Untitled-a773c2aa-c715-4ccb-8b21-66eaaf1e22b3.png" alt></p><h3 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle!!"></a>Shuffle!!</h3><p>suffle 이 일어나는 경우는 api 마다 다양할 수 있다. 가장 기본적으로, 우리가 default partition 갯수를 변경하므로써 shuffle 이 일어나는 것을 확인 할 수 있다.</p><pre><code>val data = sc.textFile(&quot;file_name&quot;)data.getNumPartitions// Partition 의 숫자를 확인해보면, default 이므로 2 인 것을 확인 할 수 있다.val newData = data.repartition(10)newData.getNumPartitions// Partition 갯수가 10로 변경된 것을 확인 할 수 있다.newData.toDebugString// newData 의 Lineage 를 확인하면, repartition 이 일어나면서 shuffle 이 되고,// shuffle 로 인해 stage 가 2개가 되는 것을 확인 할 수 있다.(indentation)newData.count// action api 를 수행하여 앞선 command 를 모두 수행</code></pre><p><img src="Untitled-95d981bb-62a7-457f-9378-0b41db4d5b42.png" alt></p><p>위 command line 에 대한 DAG 를 웹 UI 를 통해 확인하면, 다음과 같이 stage 가 repartition을 기점으로 나누어 지는 것을 확인 할 수 있다.</p><p><img src="Untitled-ef367f62-d6f5-47da-9086-cdaa914aa5d7.png" alt></p><p>총 Partition의 갯수 (Task의 갯수)를 확인해 보면,  default 로 수행된 partition 2 개와, 우리가 설정해준 Partition 의 갯수인 10개를 합하여 12개인 것을 확인 할 수 있다.</p><p><img src="Untitled-6bf929e7-3f6d-4f2f-8898-04ca7ba404b6.png" alt></p><p>여기서 한 스텝을 더 들어가보면, spark 만의 특이한 특징을 확인 할 수 있다.</p><pre><code>// 위 코드에 이어서, newData 에 대해// newData RDD를 collect 해서 cli에 찍는 command 를 수행해보자.newData.collect.foreach(println)</code></pre><p>collect api 와 RDD의 element를 print 를 하는 action api 를 수행할 때, 지금까지 공부한 것으로 생각해 보면, text를 읽어서, 2개의 Partition 을 나누고, 다시 10개의 Partition 을 나누는 작업으로 이전의 12 개의 Task 와 다를게 없을 것 같은 느낌이다. 하지만 UI 를 통해 확인해보면, 10개의 Partition 으로 2개가 skipped 되었다고 확인할 수 있다. DAG 에서도, skipped 된 stage에 대해서 회색으로 확인된다.</p><p><img src="Untitled-6e461b14-8e34-4edb-91ad-6dd9ba2ba516.png" alt></p><p><img src="Untitled-85afaaf0-58d6-4533-995d-c5ec919da985.png" alt></p><p>이는 spark 에서 이 커맨드라인을 수행할 때, process 간 통신이 file 을 기반으로한 통신을 했기 때문이다. 제일 처음 <code>newData</code> 에 대해서 수행 될 때, 첫 stage 에서 shuffle 이 수행 될 때, 해당 파일을 각 executor 에서 shuffle write 을 하고 저장해두었다가, 두번째 stage 에서 shuffle  이 수행 될때, shuffle read 를 하는 방식으로 file을 기반으로 processor 가 통신하게 된다.</p><p><img src="Untitled-310cc7d4-4e31-4362-bf74-f0e046a27052.png" alt></p><p>따라서, spark 가 같은 command line 을 수행하게 되면 미리 shuffle write  된 file 을 읽기만 함으로써 앞선 stage 의 동일한 반복 작업을 수행하지 않게 되는 것이다. UI 를 확인 해보아도, shuffle read 만 수행 되었다. </p><p><img src="Untitled-e89e6c5f-89f8-475c-af4a-78c1e3ae8377.png" alt></p><h3 id="SaveFile"><a href="#SaveFile" class="headerlink" title="SaveFile!!!"></a>SaveFile!!!</h3><p><code>RDD.saveAsTextFile(&quot;directory_name&quot;)</code> api 를 활용하여, 어떤 처리가 끝난 RDD 를 저장할 수 있다. 이 때 주의 할 점은 parameter 에 들어 가는 것이 directory_name 이라는 것이다. 또한 partition 별로 파일이 저장된다. (e.g. 10개의 partition 이라면, 10개의 file이 저장된다.)</p><h3 id="Cache"><a href="#Cache" class="headerlink" title="Cache!!!"></a>Cache!!!</h3><p>spark가 자랑하는 가장 큰 특징은, data(RDD) 를 memory에 cache  함으로써 처리의 속도가 매우 빠르다는 점이다. <code>RDD.cache</code> api 를 통해 memory 에 캐시할 수 있다. </p><pre><code>// distData RDD 에 이름을 부여distData.name = &quot;myData&quot;// cache!distData.cache// action : 5 개의 data 를 가져옴distData.take(5)// action : collectdistData.collect</code></pre><p><code>distData.take(5)</code> 까지 한 결과를 UI 에서 cache 를 살펴보면, 다음과 같다.</p><p><img src="Untitled-54196c66-b41c-4216-af72-5a70b030b321.png" alt></p><p>우리가 설정 한 것 처럼, RDD 의 이름이 myData 로 들어간것을 확인 할 수 있고 cache  역시 확인 할 수 있다. 하지만, Cached  된 비율을 확인하면 전체 RDD  에서 50% 만 된 것을 확인 할 수 있다. 반면에, <code>distData.collect</code> action 을 취하게 되면, Fraction Cached  가 100% 가 된 것을 확인 할 수 있다.</p><p><img src="Untitled-8f50bedc-ace7-4c09-9450-fb277ee5830b.png" alt></p><p>이는 우리의 action 에 따라 cache 할 용량이 달라 질 수 있기 때문이다. spark 입장에서 take(5) api 는 전체 RDD 중 5개의 element data 만 가져오면되고, 이 때 2개의 Partition 중 하나의 Partition 만 cache 해도 충분하기 때문에 Fraction Cached가 50%라고 나오는 것이다. 반면 collect api 는 collect 자체가 각 executor 에 있는 data 를 driver 로 모두 가져오는 것이므로 100% cache 하게 된다.</p><p><strong>Cache 에서 중요한 것은, 각 executor 의 cache 를 위한 가용 메모리 공간이 해당 Partition의 용량보다 작을 경우, 저장 할 수 있는 용량만큼 저장되는 것이 아니라, 해당 Partition 은 아예 저장이 안되게 된다. 이 점은 Cache를 할 때, Partition 의 용량과 해당 Executor 의 가용 메모리 공간을 미리 파악하여, 설계해야 한다.</strong></p><h3 id="Word-Count-예제"><a href="#Word-Count-예제" class="headerlink" title="Word Count 예제!!!"></a>Word Count 예제!!!</h3><p>우리가 데이터 분석을 할 때, 가장 basic 한 방법은 해당 데이터의 갯수를 세어 보는 것이다. 본 예제에서는 텍스트 파일을 읽어, 띄어쓰기를 바탕으로 word token을 나누고, 이를 세어보자.</p><p>WordCount 예제는 매우 basic 한 코드이므로, 어떤 로직으로 돌아가는지 완벽한 이해와 코드작성이 필수라고 생각한다.</p><pre><code>val originalDataRDD = sc.textFile(&quot;text-file&quot;)val wordcountRDD = originalDataRDD.flatMap(line =&gt; line.split(&quot; &quot;))                                        .map(word =&gt; (word, 1)).reduceByKey(_ + _)wordcountRDD.collect.foreach(println)</code></pre><ol><li>originalDataRDD 에서 text-file을 읽고,</li><li>line 마다 띄어쓰기를 기준으로 split 하고 이를 .flatMap 을 통해, flatten 하게 됩니다.</li><li>그리고 .map 을 통해 (word, 1) tuple 형태로 mapping 합니다.</li><li>.reduceByKey 를 통해 같은 word 에 대해 그 counting 갯수를 더하게 된 것을 RDD 로 return 하게 됩니다.</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Apache-Spark-Basic-1&quot;&gt;&lt;a href=&quot;#Apache-Spark-Basic-1&quot; class=&quot;headerlink&quot; title=&quot;Apache Spark Basic 1&quot;&gt;&lt;/a&gt;Apache Spark Basic 1&lt;/h1&gt;&lt;
      
    
    </summary>
    
      <category term="MachineLearning" scheme="https://emjayahn.github.io/categories/MachineLearning/"/>
    
      <category term="Apache Spark" scheme="https://emjayahn.github.io/categories/MachineLearning/Apache-Spark/"/>
    
    
  </entry>
  
  <entry>
    <title>[CS231n]Lecture05-CNN</title>
    <link href="https://emjayahn.github.io/2019/05/20/CS231n-Lecture05-Summary/"/>
    <id>https://emjayahn.github.io/2019/05/20/CS231n-Lecture05-Summary/</id>
    <published>2019-05-20T08:36:09.000Z</published>
    <updated>2019-05-20T08:40:56.716Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Lecture-05-Convolutonal-Neural-Networks"><a href="#Lecture-05-Convolutonal-Neural-Networks" class="headerlink" title="Lecture 05: Convolutonal Neural Networks"></a>Lecture 05: Convolutonal Neural Networks</h1><ul><li>이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다.</li></ul><h2 id="1-Convolution-Layer"><a href="#1-Convolution-Layer" class="headerlink" title="1. Convolution Layer"></a>1. Convolution Layer</h2><h3 id="1-1-Fully-Connected-Layer-와-Convolution-Layer-의-비교"><a href="#1-1-Fully-Connected-Layer-와-Convolution-Layer-의-비교" class="headerlink" title="1-1. Fully Connected Layer 와 Convolution Layer 의 비교"></a>1-1. Fully Connected Layer 와 Convolution Layer 의 비교</h3><p>32 x 32 x 3 image 가 있다고 하자. network 에 주입하기 위해, 1 x 3072 로 핀 data 를 상상해 보자.</p><p>Fully Connected Layer 의 경우, 아래 그림 처럼, Weight 과 dot product 가 수행되어, activation 값이 나오게 된다. 이 때, activation 의 갯수는 W 의 크기에 따른다.</p><p><img src="Untitled-c7b225ec-cc03-4a1b-900f-558beebc21b5.png" alt></p><p>Fully Connected Layer 의 경우, 사진이라는 공간적 구조(Spatial Structure) 가 중요한 data 에 대해, 공간적인 정보를 다 잃어버리는 문제가 발생한다. 공간적 정보를 보존하기 위해 Convolution Layer 를 사용한다. </p><h3 id="1-2-Convolution-Layer-Overview"><a href="#1-2-Convolution-Layer-Overview" class="headerlink" title="1-2. Convolution Layer Overview"></a>1-2. Convolution Layer Overview</h3><p><img src="Untitled-7729c470-9f6b-4d19-9656-cc46c1dfb812.png" alt></p><p>이 때, 실제 convolution 계산은 image 의 filter 크기 만큼의 matrix 를 vectorize 한 후, filter vector 와 dot product 로 수행한다고 한다. 따라서 이 때 헷갈리지 않도록 할 것은, 한번의 Convolution 연산 결과는 <strong>하나의 scalar value</strong> 가 된다. 따라서, 하나의 filter 가 이미지 한 장을 훑어 내려간다면, 원본 이미지보다는 조금 작은 depth 가 1인 <strong>activation map</strong> 이 결과로 나온다.</p><p><img src="Untitled-fce64257-258e-4b3e-93ab-d0543662984f.png" alt></p><p>따라서 activation map 의 깊이(channel 수)는, <strong>filter 의 갯수</strong> 와 동일하다. 여러개의 필터는 이미지의 각기 다른 특징을 추출하려는 의도에서 사용된다.</p><p>그 후, 이 Convolution 의 결과인 activation map 을 비선형 함수(ReLU 등)에 통과 시킨다.</p><p><strong>여러 유명한 ConvNet 들은 이렇게, Convolution Layer 와 비선형함수를 반복적으로 나열 한 Network  라고 볼수 있다</strong></p><h3 id="1-3-Convolution-Layer-의-결과물"><a href="#1-3-Convolution-Layer-의-결과물" class="headerlink" title="1-3. Convolution Layer 의 결과물"></a>1-3. Convolution Layer 의 결과물</h3><p>이렇게 여러 계층의 Convolution Layer를 쌓는 것은, 가장 아래 Layer 부터 높은 Layer 까지 단순한 feature → 복잡한 feature 를 뽑아 내는 것으로 볼 수 있다.</p><p><img src="Untitled-5fec1f58-a947-449b-8d71-772aac9a6f3e.png" alt></p><h3 id="1-4-Convolution-Layer-연산"><a href="#1-4-Convolution-Layer-연산" class="headerlink" title="1-4. Convolution Layer 연산"></a>1-4. Convolution Layer 연산</h3><p><img src="Untitled-2a8adbad-1f2a-4fac-898d-aac944438a1b.png" alt></p><p>filter 가 이미지를 훑고 지나가면서 convolution 연산을 한 후, 나온 결과는 원본 이미지보다 그 크기가 작아지게 된다. 그 정도는 filter 의 크기와 filter 가 훑고 지나가는 간격인 stride 에 따라 바뀌게 된다. </p><script type="math/tex; mode=display">output \;size = (N-F)/stride + 1</script><p><strong>문제점:</strong></p><ol><li><p>convolution 연산의 문제는 이미지의 모서리에 있는 정보는 가운데에 있는 이미지의 정보보다 적게 추출 되는 문제가 있다.(filter 가 모서리를 넘어서는 이동 할 수 없으므로)</p></li><li><p>convolution layer 를 반복적으로 지나가다 보면, map의 크기가 매우 빠르게 작아지게 된다. </p></li></ol><p>이를 위해 적용하는 것이 Padding 이다.</p><h3 id="1-5-Padding"><a href="#1-5-Padding" class="headerlink" title="1-5. Padding"></a>1-5. Padding</h3><p>모서리에 정보를 얻기 위해 이미지이 외곽에 숫자를 채워 주는 방법. 이 때, 많이 사용하는 방법은 zero-padding. zero-padding 외에도 다양한 방법이 있다.</p><h2 id="2-Pooling-Layer"><a href="#2-Pooling-Layer" class="headerlink" title="2. Pooling Layer"></a>2. Pooling Layer</h2><p>Parameter 의 갯수를 줄이기 위해, 우리가 ConvLayer 를 통해 뽑아낸 image 를 작게 만드는 Layer  이다. 즉, Downsampling 을 위한 것.</p><p>Maxpooling 의 intuition : 앞선 layer filter 가 각 region  에서 얼마나 활성 되었는지 보는 것이다. </p><h3 id="3-Typical-Architecture"><a href="#3-Typical-Architecture" class="headerlink" title="3. Typical Architecture"></a>3. Typical Architecture</h3><p>[[(Conv → RELU) <em> N → Pool] </em> M → (FC → RELU) * K ] → SOFTMAX</p><ul><li>N : ~ 5</li><li>M : Large</li><li>K : 0 ~ 2<ul><li>ResNet, Google net 등은 이 방식을 훨씬더 뛰어넘음</li></ul></li></ul><h2 id="4-Reference"><a href="#4-Reference" class="headerlink" title="4. Reference"></a>4. Reference</h2><p><a href="https://www.youtube.com/watch?v=bNb2fEVKeEo" target="_blank" rel="noopener">Lecture 5 | Convolutinoal Neural Networks</a></p><p><a href="http://cs231n.stanford.edu/syllabus.html" target="_blank" rel="noopener">Syllabus | CS 231N</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Lecture-05-Convolutonal-Neural-Networks&quot;&gt;&lt;a href=&quot;#Lecture-05-Convolutonal-Neural-Networks&quot; class=&quot;headerlink&quot; title=&quot;Lecture 05: Co
      
    
    </summary>
    
      <category term="Lecture" scheme="https://emjayahn.github.io/categories/Lecture/"/>
    
      <category term="CS231n" scheme="https://emjayahn.github.io/categories/Lecture/CS231n/"/>
    
    
      <category term="summary" scheme="https://emjayahn.github.io/tags/summary/"/>
    
      <category term="CS231n" scheme="https://emjayahn.github.io/tags/CS231n/"/>
    
  </entry>
  
  <entry>
    <title>[CS231n]Lecture04-Backprop/NeuralNetworks</title>
    <link href="https://emjayahn.github.io/2019/05/18/CS231n-Lecture04-Summary/"/>
    <id>https://emjayahn.github.io/2019/05/18/CS231n-Lecture04-Summary/</id>
    <published>2019-05-18T14:21:08.000Z</published>
    <updated>2019-05-20T08:37:22.159Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Lecture-04-Backpropagation-and-Neural-Networks"><a href="#Lecture-04-Backpropagation-and-Neural-Networks" class="headerlink" title="Lecture 04: Backpropagation and Neural Networks"></a>Lecture 04: Backpropagation and Neural Networks</h1><ul><li>이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다.</li></ul><h2 id="1-Backpropagation"><a href="#1-Backpropagation" class="headerlink" title="1. Backpropagation"></a>1. Backpropagation</h2><h3 id="1-1-핵심"><a href="#1-1-핵심" class="headerlink" title="1-1. 핵심"></a>1-1. 핵심</h3><p>Backprop의 한줄요약: 각 parameter 에 대해 Loss Function 의 gradient 를 구하기 위해 사용하는 graphical representation of <strong>ChainRule</strong></p><p>언뜻 보면 어려울 수 도 있지만, just ChainRule</p><p><img src="Untitled-13d44613-a85b-4dcc-a807-8b114b0138c4.png" alt></p><p>위의 그림을 예를 들어 살펴 보면, y 에 대한 f의 gradient 는 q에 대한 f의 gradient <em> y 에 대한 q 의 gradient 으로 볼 수 있다. 이를 해석하자면, f 에게 미치는 y 의 영향 = f 에게 미치는 q의 영향 </em> q 에게 미치는 y 의 영향으로 볼 수 있다.</p><p>Backpropagation 의 가장 중요한 특징!!</p><pre><code>gradient 를 구하기 위해 node 를 기준으로 앞과 뒤만 보면 된다.</code></pre><p><img src="Untitled-f2d8c1d9-1a28-4e16-8e35-83ff9f93045f.png" alt></p><p>또한, 위 그림을 보게 되면, Forward passing 과 마찬가지로, Backprop 시에도, 이전 노드에서 전달 되는 Gradient 를 node 에서 local gradient 와의 연산으로 다음 Node 에 gradient 를 전달해 줄 수 있다.</p><h3 id="1-2-Back-prop-시-각-node-의-역할"><a href="#1-2-Back-prop-시-각-node-의-역할" class="headerlink" title="1-2. Back prop 시, 각 node  의 역할"></a>1-2. Back prop 시, 각 node  의 역할</h3><p><img src="Untitled-7a11e194-ea07-4922-a7b5-aa4346476acb.png" alt></p><ol><li>Add gate : gradient distributor<ul><li>add gate 를 기점으로, 각 입력(forward 방향의 입력)의 gradient로 local gradient 를 구하면 1이므로, 전달 되는 gradient 와 각각 1씩 곱해져 전달 되게 된다. 이 현상을 보게 되면 동등하게 나눠주는 역할을 하므로 gradient distributor 라고 볼 수 있다.</li></ul></li><li><p>Max gate : gradient router</p><ul><li>Max gate 는 gradient 를 한쪽에는 전체, 다른 쪽에는 0 을 준다.</li><li><p>해석적으로 보자면, max 연산을 통해 forward 방향에서 영향을 준 branch 에게 gradient 를 전달해 주는 것이 합적</p><script type="math/tex; mode=display">max(x, y) = \begin{cases} x \quad\quad if \quad x > y \\\\ y \quad\quad if \quad x < y \end{cases}</script></li><li><p>수식으로 보자면, x 에 대한 gradient, y 에 대한 gradient 가 각각 (1, 0), (0, 1)  로 local gradient 가 계산되기 때문이다.</p></li><li>gradient 가 전달될 길을 결정해주는 면에서, 네트워크에서 path 를 설정해 주는 router의 기능과 비슷하다.</li></ul></li><li>Mul gate : gradient switcher + scaler<ul><li>곱셈연산의 gradient 의 경우, x 에 대한 gradient 는 y 가 되므로, 서로 바꿔주는 역할을 한다. 이 때, forward 상에서의 결과 값으로 곱해주므로, scale 역할까지 함께 하게 된다.</li></ul></li></ol><h2 id="2-Neural-Networks"><a href="#2-Neural-Networks" class="headerlink" title="2. Neural Networks"></a>2. Neural Networks</h2><p>강의에서는 Neural Network 에 대한 intuition 을 위해, biological neuron 과 비교하였다. 모델 architecture 로서의 neuron 과 biological neuron 의 공통점은 다음과 같다.</p><ol><li>input impulse</li><li>input axon → dendrite</li><li>(cell body)activation &amp; activation function</li><li>output axon</li></ol><p>이러한 비교는, 나의 개인적인 Neural Network에 대한 공부와 이해에 도움이 되지 않기에 큰 감동은 없다. </p><ul><li>4강에 대한 핵심 사항은, Backpropagation 에 대한 수식적 이해와 그 이해를 통해 Backpropagation 이 gradient를 구함에 있어, 얼마나 편한 representation 인지이다. 공부하며 어려웠던 것은, backprop in <strong>vectorized</strong> 에서, Jacobian Matrix 의 표현이 scalar  backprop 때와는 달리 한번에 머릿속으로 상상되지 않았기에, 손으로 써가며 따라 갔어야만 했다.</li></ul><h2 id="3-Reference"><a href="#3-Reference" class="headerlink" title="3. Reference"></a>3. Reference</h2><p><a href="https://www.youtube.com/watch?v=d14TUNcbn1k" target="_blank" rel="noopener">Lecture 4 | Introduction to Neural Networks</a></p><p><a href="http://cs231n.stanford.edu/syllabus.html" target="_blank" rel="noopener">Syllabus | CS 231N</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Lecture-04-Backpropagation-and-Neural-Networks&quot;&gt;&lt;a href=&quot;#Lecture-04-Backpropagation-and-Neural-Networks&quot; class=&quot;headerlink&quot; title=&quot;
      
    
    </summary>
    
      <category term="Lecture" scheme="https://emjayahn.github.io/categories/Lecture/"/>
    
      <category term="CS231n" scheme="https://emjayahn.github.io/categories/Lecture/CS231n/"/>
    
    
      <category term="summary" scheme="https://emjayahn.github.io/tags/summary/"/>
    
      <category term="CS231n" scheme="https://emjayahn.github.io/tags/CS231n/"/>
    
  </entry>
  
  <entry>
    <title>[CS231n]Lecture03-LossFunction/Optimization</title>
    <link href="https://emjayahn.github.io/2019/05/16/CS231n-Lecture03-Summary/"/>
    <id>https://emjayahn.github.io/2019/05/16/CS231n-Lecture03-Summary/</id>
    <published>2019-05-16T01:17:11.000Z</published>
    <updated>2019-05-20T08:34:01.925Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Lecture-03-Loss-Function-amp-Optimization"><a href="#Lecture-03-Loss-Function-amp-Optimization" class="headerlink" title="Lecture 03: Loss Function &amp; Optimization"></a>Lecture 03: Loss Function &amp; Optimization</h1><ul><li>이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다.</li></ul><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><ol><li>Loss Function : 우리가 가지고 있는 W matrix 가 <strong>얼마나 안좋은지 정량화(Quantify)</strong> </li><li>Optimization : 위의 Loss Function 을 minimize 해서 가장 좋은 parameter (W) 를 찾는 과정</li></ol><h2 id="2-Loss-Function"><a href="#2-Loss-Function" class="headerlink" title="2. Loss Function"></a>2. Loss Function</h2><p>주어진 data 가 다음과 같을 때, </p><script type="math/tex; mode=display">\{(x_i, y_i)\}_{i=1}^{N}</script><p> Loss 는 “Average of over examples” 즉, </p><script type="math/tex; mode=display">L = \frac{1}{N}\sum_{i}L_i(f(x_i, W), y_i)</script><ul><li>딥러닝 알고리즘의 General Setup<ul><li>W 가 얼마나 좋고, 나쁜지를 정량화하는 손실함수 만들기</li><li>W 공간을 탐색하면서  이 loss를 minimize 하는 W 를 찾기</li></ul></li></ul><h3 id="2-1-Loss-Example-Multiclass-SVM-Loss"><a href="#2-1-Loss-Example-Multiclass-SVM-Loss" class="headerlink" title="2-1. Loss Example: Multiclass SVM Loss"></a>2-1. Loss Example: Multiclass SVM Loss</h3><p>SVM Loss 는 다음과 같다. 주어진 data example (x_i, y_i) 에 대해서, score vector <strong>s</strong> 는 다음과 같다. </p><script type="math/tex; mode=display">s = f(x_i, W)</script><p>이 때, SVM loss는 </p><script type="math/tex; mode=display">L_i = \sum_{j\neq y_i}\begin{cases} 0 \quad\quad\quad\quad\quad\quad\quad if \;s_{y_i} \geq s_j +1 \\ s_j - s_{y_i} + 1 \quad\quad otherwise \end{cases} \\ = \sum_{j\neq y_i}max(0, s_j-s_{y_i}+ 1)</script><p>x_i 의 정답이 아닌 클래스의 score (s_j) + 1 (safety margin) 과 정답 클래스 score s_yi 를 비교하여, Loss 를 계산한다.</p><p><img src="Untitled-d804129d-082d-4cfc-819c-4fd3e14ddf17.png" alt></p><ol><li>SVM Loss 의 최대, 최솟값은 ? min : 0, max :  infinite</li><li>W 를 작게 초기화 하면, s 가 거의 0에 가까워 진다. 이 때, SVM Loss 는 어떻게 예상되는가?<ul><li>정답이 아닌 class, 즉 class - 1 개의 score 원소들을 순회하면서 모두 더할 때, score 는 0에 가깝고, 이를 average 취하면 <strong>class 갯수 - 1</strong> 만큼의 Loss 값이 나온다.</li><li>이 특징은 debugging strategy 로 사용할 수 있다. 초기 loss 가 C-1 에 가깝지 않으면 bug 가 있는 것으로 의심해볼 수 있다.</li></ul></li><li>만약 include j = y_i 이면, SVM Loss 는 어떻게 되는가? <ul><li>Loss Funtion 이 바뀌는 것은 아니다. 단지 전체 loss의 minimum 이 1이 될 뿐이므로 해석의 관점에서 관례상 맞지 않아 정답 class 는 빼고 계산한다.</li></ul></li><li>우리가 average 를 취하지 않으면?<ul><li>이 역시 바뀌는 것이 없다. 전체 class 수는 정해져 있고, 이를 나누는 average 는 scaling 만 할 뿐이다.</li></ul></li><li>Loss 를 max(0, s_j - s_yi + 1) ^2 를 사용하면?<ul><li>이는 squared hinge function 으로 때에 따라서 사용할 수 있는 loss function 이다. 다른 Loss function 이며, 이는 위의 loss 와 다르게 해석 할 수 있다. 기존의 SVM loss 는 class score 가 각각 얼마나 차이가 나는지에 대해서는 고려하지 않는 것이라고 한다면, squared 가 들어감으로써, 차이가 많이 나는 score class 에 대해서는 좀더 가중하여 고려하겠다는 의미로 해석 할 수 있다.</li></ul></li></ol><h3 id="2-2-Regularization"><a href="#2-2-Regularization" class="headerlink" title="2-2. Regularization"></a>2-2. Regularization</h3><p>만약 위 Loss Function 에 대해서, L = 0 으로 만드는 W 를 찾았다고 할때, 과연 이 W 는 유일한가? 그렇지 않다. W 일 때, L=0  이라면, 2W 역시 L=0 이다. 또한 L을 0으로 만드는 다양한 W 중에서 단지  training  data 에만 fit 하는 classifier 를 원하는 것이 아니라, test data에서 좋은 성능을 발휘하는 classifier 를 찾기를 원한다. 이런 Overfitting 을 막기 위해서는 모델의 W 를 다른 의미에서 조절해줄 수 있는 <strong>Regularization</strong> term 을 추가할 수 있다.</p><p><strong>즉, Model이 training set 에 완벽하게  fit 하지 못하도록 Model 의 복잡도에 penalty 를 부여하는 것을 말한다.</strong></p><p><img src="Untitled-253fa150-bbe6-473f-bd43-e3fd6fe0009d.png" alt></p><p>Regularization 의 종류들:</p><ul><li>L2 Regularization</li><li>L1 Regularization</li><li>Elastic net(L1 + L2)</li><li>Max norm Regularization</li><li>Dropout</li><li>Batch normalization, stochastic depth</li></ul><h3 id="2-3-Loss-example-Softmax-Classifier-Multinomial-Logistic-Regression"><a href="#2-3-Loss-example-Softmax-Classifier-Multinomial-Logistic-Regression" class="headerlink" title="2-3 Loss example: Softmax Classifier (Multinomial Logistic Regression)"></a>2-3 Loss example: Softmax Classifier (Multinomial Logistic Regression)</h3><p>deeplearning 에서 훨씬 더 자주 보게 되는 loss 의 종류 중 하나이다.  위에서 살펴본 SVM loss 의 단점은 그 값 자체에 어떤 의미를 부여하기는 힘들다는 점이다. 반면에, Softmax Classifier 는 그 값 자체를 확률적 해석이 가능하기 때문이다. (cf. 콜모고로프의 공리를 통해 softmax 의 layer 의 output 이 확률로 해석 될 수 있다.)</p><p>Softmax Function 은 다음과 같다.</p><script type="math/tex; mode=display">P(Y=k|X=x_i) = \frac{e^{s_k}}{\sum_j e^{s_j}},\quad where \quad s = f(x_i;W)</script><h2 id="3-Opitmization"><a href="#3-Opitmization" class="headerlink" title="3. Opitmization"></a>3. Opitmization</h2><p>Optimization 을 한마디로 요약하자면, <strong>우리의  loss 를 최소화 하는 W 를 찾기</strong> 가 되겠다. 그 방법에는,</p><ul><li>(바보 같은 접근인: 강의표현) Random Search</li><li>Gradient 를 구하는 방법<ul><li>Numerical Gradient 수치적 접근 : 이 방법은 근사치를 구하는 것이며, 매우 느린 단점이 있다. 하지만, 쉽게 작성할 수 있다는 장점이 있다.</li><li>Analytic Gradient 해석적 접근 : 미분식을 구해야하는 단점이 있다. 하지만 빠르고 정확하다.</li></ul></li></ul><p>실제로는, Analytic Gradient 방법을 사용한다. 하지만 debugging 을 위해 numerical gradient 를 사용한다. 이를 <strong>gradient check</strong>이라 한다.</p><h3 id="3-1-Gradient-Descent-amp-Stochastic-Gradient-Descent"><a href="#3-1-Gradient-Descent-amp-Stochastic-Gradient-Descent" class="headerlink" title="3-1. Gradient Descent &amp; Stochastic Gradient Descent"></a>3-1. Gradient Descent &amp; Stochastic Gradient Descent</h3><p>Gradient Descent 를 방법을 이용해서 optimization 을 진행할 수 있다. 하지만 데이터의 숫자와 차원이 매우 큰 경우, parameter (W) 를 update 하는데 그 연산량이 매우 큰 단점과 위험이 있다. 이를 해결하기 위해 minibatch 를 사용하여 확률적 접근을 사용한다.</p><h2 id="4-Image-Feature-Extraction"><a href="#4-Image-Feature-Extraction" class="headerlink" title="4. Image Feature Extraction"></a>4. Image Feature Extraction</h2><p>CNN 등이 등장하기 전에 Image 에서  Feature 를 뽑아내는 방법에 대해 소개한다. Feature를 뽑아내는 개념으로 생각할 수 도 있지만, Feature Transform 이라는 표현을 사용한다. </p><ol><li>Color Histogram :  이미지의 color distribution 을 사용하여 해당 이미지의 feature 로 사용할 수 있다. (출처: <a href="https://en.wikipedia.org/wiki/Color_histogram" target="_blank" rel="noopener">wikipedia</a> ) For digital images, a color histogram represents the number of pixels that have colors in each of a fixed list of color ranges, that span the image’s color space, the set of all possible colors.</li><li>Histogram of Oriented Gradients (HoG) : CNN 이 등장하기 전, 매우 인기있는 Image Feature 중 하나라고 알고 있다. Edge 를 검출하는 방법이다. pixel 사이에, 값의 gradient 가 가장 큰 neighbor 가 edge 일 것이다라는 개념을 사용하여 edge 를 검출한다. 사진을 8 x 8 patch 를 만들어, 각 patch 마다 9 directional oriented gradients 를 계산하여, 이를 feature 로 사용하는 방법이다.</li><li>Bag of Words : NLP 에서도 자주 사용되는 개념인 BoW 에서 차용한 개념으로, 이미지 데이터들에서 일정 크기의 patch 를 모아 clustering 을 통해 visual words (codebook)  을 만든다. 그리고 feature 뽑아내고 싶은 image 를 patch 형태로 바꾸고, codebook 에서 찾아 histogram 을 만들어 이를 feature 로 사용한다.</li></ol><h2 id="5-Reference"><a href="#5-Reference" class="headerlink" title="5. Reference"></a>5. Reference</h2><p><a href="https://www.youtube.com/watch?v=h7iBpEHGVNc" target="_blank" rel="noopener">Lecture 3 | Loss Functions and Optimization</a></p><p><a href="http://cs231n.stanford.edu/syllabus.html" target="_blank" rel="noopener">Syllabus | CS 231N</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Lecture-03-Loss-Function-amp-Optimization&quot;&gt;&lt;a href=&quot;#Lecture-03-Loss-Function-amp-Optimization&quot; class=&quot;headerlink&quot; title=&quot;Lecture 03
      
    
    </summary>
    
      <category term="Lecture" scheme="https://emjayahn.github.io/categories/Lecture/"/>
    
      <category term="CS231n" scheme="https://emjayahn.github.io/categories/Lecture/CS231n/"/>
    
    
      <category term="summary" scheme="https://emjayahn.github.io/tags/summary/"/>
    
      <category term="CS231n" scheme="https://emjayahn.github.io/tags/CS231n/"/>
    
  </entry>
  
  <entry>
    <title>[CS231n]Lecture02-Image Classification Pipeline</title>
    <link href="https://emjayahn.github.io/2019/05/13/CS231n-Lecture02-Summary/"/>
    <id>https://emjayahn.github.io/2019/05/13/CS231n-Lecture02-Summary/</id>
    <published>2019-05-13T03:46:04.000Z</published>
    <updated>2019-05-20T08:32:45.622Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Lecture-02-Image-Classification-Pipeline"><a href="#Lecture-02-Image-Classification-Pipeline" class="headerlink" title="Lecture 02: Image Classification Pipeline"></a>Lecture 02: Image Classification Pipeline</h1><ul><li>이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다.</li></ul><h2 id="1-Image-Classification-의-기본-TASK"><a href="#1-Image-Classification-의-기본-TASK" class="headerlink" title="1. Image Classification 의 기본 TASK"></a>1. Image Classification 의 기본 TASK</h2><p><img src="Untitled-dfb244ee-00d4-4801-9db3-2247269d65db.png" alt></p><ul><li>위 사진을 보고, → ‘CAT’ 혹은 ‘고양이’ 라고 classification</li><li>자연스럽게 따라오는 문제는 <strong>“Sementic Gap”</strong> : 우리가 준 data (pixel 값  [0, 255]) 와 Label 간의  gap</li><li>또한, 이 과정에서 극복해야 하는 Challenges<ul><li>Viewpoint Variation ( 같은 객체에 대해 시점이 이동해도 robust)</li><li>Illumination ( 빛, 밝기, 명암 등에도 robust)</li><li>Deformation ( 다양한 Position, 형태의 변형에도 robust)</li><li>Occlusion ( 다른 물체나 환경에 의해 가려지는 data 에도 robust)</li><li>Background Clutter ( 배경과 비슷하게 보이는 객체에도 robust)</li><li>Intraclass Variation ( 한 종류의 클랫스에도 다양한 색과 모습의 객체가 있을 수 있다.)</li></ul></li></ul><h2 id="2-기존의-시도와-New-Era"><a href="#2-기존의-시도와-New-Era" class="headerlink" title="2. 기존의 시도와 New Era"></a>2. 기존의 시도와 New Era</h2><ul><li>Hard Coded Algorithm 과 여러 규칙 (rule-based로 해석된다) 들을 통해서 Image를 Classify 하는 노력들이 있어왔다. 이들의 문제는, (1) 위에 언급한 문제들이 Robust  하지 않다. (2) 객체가 달라지면, (고양이, 호랑이, 비행기 등) 객체마다 다 다른 규칙을 성립해줘야 한다. 즉 한마디로 요약하자면, Algorithm의 확장성이 없다.</li><li>이런 문제에 좀더 강한 방법이 지금 우리가 공부하고 있는, <strong>Data-Driven Approach</strong><ol><li>Image 와 Label pair 의 dataset 을 모은다.</li><li>Machine Learning 알고리즘을 이용해 classifier 를 학습시킨다.</li><li>Classifier 를 new images 에 테스트에 평가한다.</li></ol></li></ul><h2 id="3-First-Classifier-Nearest-Neighbor"><a href="#3-First-Classifier-Nearest-Neighbor" class="headerlink" title="3. First Classifier : Nearest Neighbor"></a>3. First Classifier : Nearest Neighbor</h2><h3 id="3-1-Nearest-Neighbor-의-기본-알고리즘"><a href="#3-1-Nearest-Neighbor-의-기본-알고리즘" class="headerlink" title="3-1. Nearest Neighbor 의 기본 알고리즘"></a>3-1. Nearest Neighbor 의 기본 알고리즘</h3><ol><li>train set 에서의 모든 data 와 label 을 기억한다.</li><li>test Image 와 <strong>가장 가까운</strong> train Image 의 label로 test image를 predict 한다.<ul><li>이 때 ‘<strong>가장 가까운’</strong> 을 계산 할 때<strong>, L1 distance</strong> 와 <strong>L2 distance</strong> 가 쓰일 수 있다. 이 외에도, 다양한 distance 지표가 쓰일 수 있다.</li></ul></li></ol><h3 id="3-2-Nearest-Neighbor-Classifier-의-문제"><a href="#3-2-Nearest-Neighbor-Classifier-의-문제" class="headerlink" title="3-2. Nearest Neighbor Classifier 의 문제"></a>3-2. Nearest Neighbor Classifier 의 문제</h3><ul><li>이미지 classification 에는 잘 사용되지 않는다.</li><li>train 보다 predict 하는데 훨씬 오래 걸린다.<ul><li>train 은 train data set 의 기억만 하면 되지만, predict 할 때는 전체 train data 에 대해 거리를 측정해야하고, sorting 해야하는 문제가 발생한다.</li><li>Time Complexity - train O(1), predict O(N) (N은 train data 수)</li></ul></li></ul><p><img src="Untitled-2fdb78f0-11b7-49cf-99f8-164d6b79fc2c.png" alt></p><p>위 그림을 보면, 연두색 공간에 노란색  class 가 포함 되어 있는 것을 볼 수 있다. 이는 generalize 면에서 부족한 모델이라고 볼 수 있다. 같은 알고리즘 이지만, 이를 해결 하는 방법은 K 개의 가까운 neighbor 로 부터 majority voting 을 받은 것으로 classify 를 하는 것이다.</p><h3 id="3-3-K-Nearest-Neightbors"><a href="#3-3-K-Nearest-Neightbors" class="headerlink" title="3-3. K-Nearest Neightbors"></a>3-3. K-Nearest Neightbors</h3><p>Single Nearest 만 보는 것이 아니라, K 개의 가까운 point 의 투표를 통해 해당 test data  의 label 을 예측한다.</p><ul><li>이 때, Voting 하는 방법에는 majority voting ( 다수결 ) 과 weighted voting ( 가중치를 주어 투표: distance 가 가까운 것에 가중치를 준다.)</li><li>가중치를 주는 방법에는 distance 가 커지면 곱해지는 weight 을 줄이는 방법으로 1 / (1+distance) 등을 weight 을 곱해준다.</li></ul><p><img src="Untitled-469c81dd-01a3-4934-8fec-429c34285d0e.png" alt></p><h3 id="3-4-k-Nearest-Neighbor-on-images-NEVER-USED"><a href="#3-4-k-Nearest-Neighbor-on-images-NEVER-USED" class="headerlink" title="3-4. k-Nearest Neighbor on images NEVER USED"></a>3-4. k-Nearest Neighbor on images NEVER USED</h3><ul><li>차원의 저주 문제</li><li>knn 이 잘 동작하기 위해서는 dataset 공간을 조밀하게 커버할 만큼의 충분한 training space 가 필요하다. 하지만, data 의 차원이 늘어날 수록 그 충분한 data 의 수가 exponential 하게 늘어난다.</li></ul><h2 id="4-Setting-Hyperparameters"><a href="#4-Setting-Hyperparameters" class="headerlink" title="4. Setting Hyperparameters"></a>4. Setting Hyperparameters</h2><p>Model 최적의 hyperparameter 를 찾기 위해서는 data set 을 구분하여, unseened data 를 사용하여 성능 검증을 하고, model selection 을 해야한다. 이는 단순이 hyperparameter 를 찾는 용도 뿐만 아니라 우리가 세운 가설을 서로 비교 할 때는 data set 을 정확히 구분하고, test set 을 통해 비교하고, 선택해야한다. 그 방법에는 train, validation, test set 으로 dataset 을 나누는 방법과 cross validation 방법이 있다.</p><ul><li>첫 번째 방법으로는, Validation set 을 통해 hyperparameter(가설)를 검증하고 선택하여, Test set 을 사용하여 Evaluate 과 Reporting 등을 한다. 딥러닝 모델링에서는 이 방법으로 많이 사용한다.</li></ul><p><img src="Untitled-7d99fc52-7613-4bbf-8cfc-5e036f905602.png" alt></p><ul><li>두 번째 방법은, data set 의 크기가 크지 않을 때, Train set 안에서 folds 들을 나누어 각 fold 가 돌아가며 validation set 이 되며, 이들의 평균값으로 가설을 비교한다. 이는 딥러닝 모델에서는 적합하지 않은 형태이다. 모델 자체의 연산이 많은데다가, 같은 모델에 대해 많은 validation 이 효율적이지 않기 때문이다. 또한 data가 많지 않은 상태에서 딥러닝 모델을 선택하는 것은 옳지 않다.</li></ul><p><img src="Untitled-1e7d1cc8-fcee-4215-a721-345c8f25e8f5.png" alt></p><h2 id="5-Second-Classifier-Linear-Classifier"><a href="#5-Second-Classifier-Linear-Classifier" class="headerlink" title="5. Second Classifier : Linear Classifier"></a>5. Second Classifier : Linear Classifier</h2><p>Linear Classifier 는 Neural Network 의 기본 골격이다. (1) image data 와 W (parameters or weights) 을 통해 연산을 해주고, (2) function 을 통과해 Classification 을 해준다.</p><p>특히 Linear Classifier 의 경우 아래 와 같이, (1) image data 와 W 를 dot product 를 해주고 (2) linear function f 를 통과한다.</p><script type="math/tex; mode=display">f(x, W) = Wx</script><p><img src="Untitled-c15a20fd-b906-44ce-af25-6810bc57751a.png" alt></p><h2 id="6-Reference"><a href="#6-Reference" class="headerlink" title="6. Reference"></a>6. Reference</h2><p><a href="https://www.youtube.com/watch?v=OoUX-nOEjG0&amp;list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&amp;index=2" target="_blank" rel="noopener">Lecture 2 | Image Classification</a></p><p><a href="http://cs231n.stanford.edu/syllabus.html" target="_blank" rel="noopener">Syllabus | CS 231N</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Lecture-02-Image-Classification-Pipeline&quot;&gt;&lt;a href=&quot;#Lecture-02-Image-Classification-Pipeline&quot; class=&quot;headerlink&quot; title=&quot;Lecture 02: 
      
    
    </summary>
    
      <category term="Lecture" scheme="https://emjayahn.github.io/categories/Lecture/"/>
    
      <category term="CS231n" scheme="https://emjayahn.github.io/categories/Lecture/CS231n/"/>
    
    
      <category term="summary" scheme="https://emjayahn.github.io/tags/summary/"/>
    
      <category term="CS231n" scheme="https://emjayahn.github.io/tags/CS231n/"/>
    
  </entry>
  
  <entry>
    <title>Basic Classification with Pytorch</title>
    <link href="https://emjayahn.github.io/2019/05/06/Basic-Classification-with-Pytorch/"/>
    <id>https://emjayahn.github.io/2019/05/06/Basic-Classification-with-Pytorch/</id>
    <published>2019-05-06T03:39:33.000Z</published>
    <updated>2019-05-06T03:40:52.489Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Basic-Classification-with-Pytorch"><a href="#Basic-Classification-with-Pytorch" class="headerlink" title="Basic Classification with Pytorch"></a>Basic Classification with Pytorch</h1><ul><li>이번 post 는 pytorch 를 활용해 기초적인 분류 모델링을 해보면서, pytorch에 익숙함을 높이는 것이 목적입니다.</li></ul><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> torch</span><br><span class="line"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn</span><br><span class="line"><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F</span><br><span class="line"><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="hljs-string">"ignore"</span>)</span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="hljs-string">'retina'</span></span><br></pre></td></tr></table></figure><h2 id="1-Binary-Classification"><a href="#1-Binary-Classification" class="headerlink" title="1. Binary Classification"></a>1. Binary Classification</h2><ol><li>Modeling</li><li>Sigmoid</li><li>Loss : Binary Cross Entropy</li></ol><h3 id="1-1-Generate-Data"><a href="#1-1-Generate-Data" class="headerlink" title="1.1 Generate Data"></a>1.1 Generate Data</h3><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># plotting function</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_scatter</span><span class="hljs-params">(W_, xy, labels)</span>:</span></span><br><span class="line">    <span class="hljs-keyword">for</span> k, color <span class="hljs-keyword">in</span> [(<span class="hljs-number">0</span>, <span class="hljs-string">'b'</span>), (<span class="hljs-number">1</span>, <span class="hljs-string">'r'</span>)]:</span><br><span class="line">        idx = labels.flatten() == k</span><br><span class="line">        plt.scatter(xy[idx, <span class="hljs-number">0</span>], xy[idx, <span class="hljs-number">1</span>], c=color)</span><br><span class="line">        </span><br><span class="line">    x1 = np.linspace(<span class="hljs-number">-0.1</span>, <span class="hljs-number">1.1</span>)</span><br><span class="line">    x2 = -W_[<span class="hljs-number">1</span>] / W_[<span class="hljs-number">2</span>] * x1 - W_[<span class="hljs-number">0</span>] / W_[<span class="hljs-number">2</span>]</span><br><span class="line">    plt.plot(x1, x2, <span class="hljs-string">'--k'</span>)</span><br><span class="line">    </span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># Generate data</span></span><br><span class="line"></span><br><span class="line">W = np.array([<span class="hljs-number">-4.</span>/<span class="hljs-number">5</span>, <span class="hljs-number">3.</span>/<span class="hljs-number">4.</span>, <span class="hljs-number">1.0</span>])</span><br><span class="line"></span><br><span class="line">xy = np.random.rand(<span class="hljs-number">30</span>, <span class="hljs-number">2</span>)</span><br><span class="line">labels = np.zeros(len(xy))</span><br><span class="line">labels[W[<span class="hljs-number">0</span>] + W[<span class="hljs-number">1</span>] * xy[:, <span class="hljs-number">0</span>] + W[<span class="hljs-number">2</span>] * xy[:, <span class="hljs-number">1</span>] &gt; <span class="hljs-number">0</span>] = <span class="hljs-number">1</span></span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_scatter(W, xy, labels)</span><br></pre></td></tr></table></figure><p><img src="output_6_0.png" alt="png"></p><h3 id="1-2-Train-data"><a href="#1-2-Train-data" class="headerlink" title="1.2 Train data"></a>1.2 Train data</h3><ul><li>Generate 한 data 로 부터, x 축 값, y 축 값, augmented term 으로 3가지 column 을 만들어 train data 로 만들어 줍니다.</li><li>또한 대응 되는 label 도 model 에 적합한 모양으로 바꾸어 줍니다.</li></ul><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_train = torch.FloatTensor([[<span class="hljs-number">1.0</span>, xval, yval] <span class="hljs-keyword">for</span> xval, yval <span class="hljs-keyword">in</span> xy])</span><br><span class="line">y_train = torch.FloatTensor(labels).view(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>)</span><br><span class="line">print(x_train[:<span class="hljs-number">5</span>])</span><br><span class="line">print(y_train[:<span class="hljs-number">5</span>])</span><br></pre></td></tr></table></figure><pre><code>tensor([[1.0000, 0.0192, 0.6049],        [1.0000, 0.0485, 0.2529],        [1.0000, 0.2412, 0.9115],        [1.0000, 0.9764, 0.1665],        [1.0000, 0.9021, 0.5825]])tensor([[0.],        [0.],        [1.],        [1.],        [1.]])</code></pre><h3 id="1-3-Modeling"><a href="#1-3-Modeling" class="headerlink" title="1.3 Modeling"></a>1.3 Modeling</h3><ul><li>Linear Model 형태와 Sigmoid 함수, Loss function 은 cross entropy 를 활용해 모델링을 합니다.</li><li>여기선, 내장되어있는 함수들을 되도록 사용하지 않고, Low level 로 코드를 작성해 보겠습니다.</li></ul><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># Low level modeling</span></span><br><span class="line">parameter_W = torch.FloatTensor([[<span class="hljs-number">-0.5</span>, <span class="hljs-number">0.7</span>, <span class="hljs-number">1.8</span>]]).view(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>)</span><br><span class="line">parameter_W.requires_grad_(<span class="hljs-keyword">True</span>)</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD([parameter_W], lr=<span class="hljs-number">0.01</span>)</span><br><span class="line"></span><br><span class="line">epochs = <span class="hljs-number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, epochs + <span class="hljs-number">1</span>):</span><br><span class="line">    <span class="hljs-comment"># Prediction</span></span><br><span class="line">    y_hat = F.sigmoid(torch.matmul(x_train, parameter_W))</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># Loss function</span></span><br><span class="line">    loss = (-y_train * torch.log(y_hat) - (<span class="hljs-number">1</span> - y_train) * torch.log((<span class="hljs-number">1</span> - y_hat))).sum().mean()</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># Backprop &amp; update</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">if</span> epoch % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:</span><br><span class="line">        print(<span class="hljs-string">"epoch &#123;&#125; -- loss &#123;&#125;"</span>.format(epoch, loss.data))</span><br></pre></td></tr></table></figure><pre><code>epoch 1000 -- loss 6.368619441986084epoch 2000 -- loss 4.5249152183532715epoch 3000 -- loss 3.654862403869629epoch 4000 -- loss 3.122910261154175epoch 5000 -- loss 2.7545464038848877epoch 6000 -- loss 2.4800000190734863epoch 7000 -- loss 2.2651939392089844epoch 8000 -- loss 2.091233253479004epoch 9000 -- loss 1.9466761350631714epoch 10000 -- loss 1.8241318464279175</code></pre><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameter_W.data.numpy()</span><br></pre></td></tr></table></figure><pre><code>array([[-16.748823],       [ 16.618748],       [ 17.622692]], dtype=float32)</code></pre><ul><li>아래 그림을 보면, Train이 잘 된 것을 알 수 있습니다.</li></ul><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_scatter(parameter_W.data.numpy(), xy, labels)</span><br></pre></td></tr></table></figure><p><img src="output_13_0.png" alt="png"></p><h2 id="2-Multiclass-Classification"><a href="#2-Multiclass-Classification" class="headerlink" title="2. Multiclass Classification"></a>2. Multiclass Classification</h2><h3 id="2-1-Generate-Data"><a href="#2-1-Generate-Data" class="headerlink" title="2.1 Generate Data"></a>2.1 Generate Data</h3><ul><li>이번에는 3개의 label 을 가지고 있는 classification 을 Modeling 해 보겠습니다.</li><li>또한, High Level 로 pytorch 의 추상 클래스를 이용해 모델링 해보겠습니다.</li></ul><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_scatter</span><span class="hljs-params">(W1, W2, xy, labels)</span>:</span></span><br><span class="line">    <span class="hljs-keyword">for</span> k, color <span class="hljs-keyword">in</span> [(<span class="hljs-number">0</span>, <span class="hljs-string">'b'</span>), (<span class="hljs-number">1</span>, <span class="hljs-string">'r'</span>), (<span class="hljs-number">2</span>, <span class="hljs-string">'y'</span>)]:</span><br><span class="line">        idx = labels.flatten() == k</span><br><span class="line">        plt.scatter(xy[idx, <span class="hljs-number">0</span>], xy[idx, <span class="hljs-number">1</span>], c=color)</span><br><span class="line">        </span><br><span class="line">    x1 = np.linspace(<span class="hljs-number">-0.6</span>, <span class="hljs-number">1.6</span>)</span><br><span class="line">    x2 = -W1[<span class="hljs-number">1</span>] / W1[<span class="hljs-number">2</span>] * x1 - W1[<span class="hljs-number">0</span>] / W1[<span class="hljs-number">2</span>]</span><br><span class="line">    x3 = -W2[<span class="hljs-number">1</span>] / W2[<span class="hljs-number">2</span>] * x1 - W2[<span class="hljs-number">0</span>] / W2[<span class="hljs-number">2</span>]</span><br><span class="line">    plt.plot(x1, x2, <span class="hljs-string">'--k'</span>)</span><br><span class="line">    plt.plot(x1, x3, <span class="hljs-string">'--k'</span>)</span><br><span class="line">    </span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># Generate data</span></span><br><span class="line"></span><br><span class="line">W1 = np.array([<span class="hljs-number">-1</span>, <span class="hljs-number">3.</span>/<span class="hljs-number">4.</span>, <span class="hljs-number">1.0</span>])</span><br><span class="line">W2 = np.array([<span class="hljs-number">-1.</span>/<span class="hljs-number">5</span>, <span class="hljs-number">3.</span>/<span class="hljs-number">4.</span>, <span class="hljs-number">1.0</span>])</span><br><span class="line"></span><br><span class="line">xy = <span class="hljs-number">2</span> * np.random.rand(<span class="hljs-number">100</span>, <span class="hljs-number">2</span>) - <span class="hljs-number">0.5</span></span><br><span class="line">labels = np.zeros(len(xy))</span><br><span class="line">labels[(W1[<span class="hljs-number">0</span>] + W1[<span class="hljs-number">1</span>] * xy[:, <span class="hljs-number">0</span>] + W1[<span class="hljs-number">2</span>] * xy[:, <span class="hljs-number">1</span>] &gt; <span class="hljs-number">0</span>)] = <span class="hljs-number">1</span></span><br><span class="line">labels[(W2[<span class="hljs-number">0</span>] + W2[<span class="hljs-number">1</span>] * xy[:, <span class="hljs-number">0</span>] + W2[<span class="hljs-number">2</span>] * xy[:, <span class="hljs-number">1</span>] &lt; <span class="hljs-number">0</span>)] = <span class="hljs-number">2</span></span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_scatter(W1, W2, xy, labels)</span><br></pre></td></tr></table></figure><p><img src="output_18_0.png" alt="png"></p><h3 id="2-2-Train-data"><a href="#2-2-Train-data" class="headerlink" title="2.2 Train data"></a>2.2 Train data</h3><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_train = torch.FloatTensor([[<span class="hljs-number">1.0</span>, xval, yval] <span class="hljs-keyword">for</span> xval, yval <span class="hljs-keyword">in</span> xy])</span><br><span class="line">y_train = torch.LongTensor(labels)</span><br><span class="line">print(x_train[<span class="hljs-number">-5</span>:])</span><br><span class="line">print(y_train[<span class="hljs-number">-5</span>:])</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 1.0000,  0.9641,  1.3851],        [ 1.0000, -0.4445,  1.0595],        [ 1.0000,  1.0854, -0.1216],        [ 1.0000,  0.8707,  0.1640],        [ 1.0000,  0.7043,  1.3483]])tensor([1, 0, 0, 0, 1])</code></pre><h3 id="2-3-Modeling"><a href="#2-3-Modeling" class="headerlink" title="2.3 Modeling"></a>2.3 Modeling</h3><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MultiModel</span><span class="hljs-params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.linear = nn.Linear(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span></span><br><span class="line">        <span class="hljs-keyword">return</span> self.linear(x)</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">model = MultiModel()</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="hljs-number">0.01</span>)</span><br><span class="line"></span><br><span class="line">epochs = <span class="hljs-number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, epochs + <span class="hljs-number">1</span>):</span><br><span class="line">    y_hat = model(x_train)</span><br><span class="line">    </span><br><span class="line">    loss = F.cross_entropy(y_hat, y_train)</span><br><span class="line">    </span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">if</span> epoch % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:</span><br><span class="line">        print(<span class="hljs-string">"epoch &#123;&#125; -- loss &#123;&#125;"</span>.format(epoch, loss.data))</span><br></pre></td></tr></table></figure><pre><code>epoch 1000 -- loss 0.6635385155677795epoch 2000 -- loss 0.5513403415679932epoch 3000 -- loss 0.4890784025192261epoch 4000 -- loss 0.44675758481025696epoch 5000 -- loss 0.4151267111301422epoch 6000 -- loss 0.39017024636268616epoch 7000 -- loss 0.369760125875473epoch 8000 -- loss 0.35262930393218994epoch 9000 -- loss 0.3379631042480469epoch 10000 -- loss 0.3252090811729431</code></pre><h3 id="2-4-Accuracy-계산"><a href="#2-4-Accuracy-계산" class="headerlink" title="2.4 Accuracy 계산"></a>2.4 Accuracy 계산</h3><ul><li>Accuracy 가 96 % 로 비교적 잘 분류 된 것을 확인 할 수 있습니다.</li></ul><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">accuracy = (torch.ByteTensor(model(x_train).max(dim=<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>] == y_train)).sum().item() / len(y_train)</span><br><span class="line">print(<span class="hljs-string">"Accuracy: &#123;&#125;"</span>.format(accuracy))</span><br></pre></td></tr></table></figure><pre><code>Accuracy: 0.96</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Basic-Classification-with-Pytorch&quot;&gt;&lt;a href=&quot;#Basic-Classification-with-Pytorch&quot; class=&quot;headerlink&quot; title=&quot;Basic Classification with 
      
    
    </summary>
    
      <category term="MachineLearning" scheme="https://emjayahn.github.io/categories/MachineLearning/"/>
    
      <category term="Pytorch" scheme="https://emjayahn.github.io/categories/MachineLearning/Pytorch/"/>
    
    
  </entry>
  
  <entry>
    <title>Linear Model with Pytorch</title>
    <link href="https://emjayahn.github.io/2019/05/04/Linear-Model-with-Pytorch/"/>
    <id>https://emjayahn.github.io/2019/05/04/Linear-Model-with-Pytorch/</id>
    <published>2019-05-04T14:54:06.000Z</published>
    <updated>2019-05-05T13:12:31.853Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Linear-Model-with-Pytorch"><a href="#Linear-Model-with-Pytorch" class="headerlink" title="Linear Model with Pytorch"></a>Linear Model with Pytorch</h1><ul><li>이 글의 목적은, 지난 Linear Regression 에서 좀더 나아가서, 다양한 Regression 예제들을 Linear Model (WX) 형태로 pytorch 를 이용해 풀어 보는 것입니다.</li><li>Pytorch 를 사용하여 Modeling 과 loss function 등을 class 형태, 내장 loss 함수등을 사용해보겠습니다.</li></ul><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> torch</span><br><span class="line"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn</span><br><span class="line"><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim</span><br><span class="line"><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="hljs-string">"ignore"</span>)</span><br><span class="line">%config InlineBackend.figure_format = <span class="hljs-string">'retina'</span></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h2 id="1-Quadratic-Regression-Model"><a href="#1-Quadratic-Regression-Model" class="headerlink" title="1. Quadratic Regression Model"></a>1. Quadratic Regression Model</h2><script type="math/tex; mode=display">f(x) = w_0 + w_1x + w_2x^2</script><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = np.linspace(<span class="hljs-number">-10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">100</span>)</span><br><span class="line">y = x**<span class="hljs-number">2</span> + <span class="hljs-number">0.7</span> * x + <span class="hljs-number">3.0</span> + <span class="hljs-number">20</span> * np.random.rand(len(x))</span><br><span class="line"></span><br><span class="line">plt.plot(x, y, <span class="hljs-string">'o'</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.xlabel(<span class="hljs-string">'x'</span>)</span><br><span class="line">plt.ylabel(<span class="hljs-string">'y'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_4_0.png" alt="png"></p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x_train = torch.FloatTensor([[each_x**<span class="hljs-number">2</span>, each_x, <span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> each_x <span class="hljs-keyword">in</span> x])</span><br><span class="line">y_train = torch.FloatTensor(y)</span><br><span class="line"></span><br><span class="line">print(<span class="hljs-string">"x_train shape: "</span>, x_train.shape)</span><br><span class="line">print(<span class="hljs-string">"y_train shape: "</span>, y_train.shape)</span><br></pre></td></tr></table></figure><pre><code>x_train shape:  torch.Size([100, 3])y_train shape:  torch.Size([100])</code></pre><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">W = torch.zeros(<span class="hljs-number">3</span>, requires_grad=<span class="hljs-keyword">True</span>)</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD([W], lr=<span class="hljs-number">0.0001</span>)</span><br><span class="line"></span><br><span class="line">epochs = <span class="hljs-number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, epochs + <span class="hljs-number">1</span>):</span><br><span class="line">    hypothesis = x_train.matmul(W)</span><br><span class="line">    </span><br><span class="line">    loss = torch.mean((hypothesis - y_train) ** <span class="hljs-number">2</span>)</span><br><span class="line">    </span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">if</span> epoch % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:</span><br><span class="line">        print(<span class="hljs-string">"epoch: &#123;&#125; -- Parameters: W: &#123;&#125; -- loss &#123;&#125;"</span>.format(epoch, W.data, loss.data))</span><br></pre></td></tr></table></figure><pre><code>epoch: 1000 -- Parameters: W: tensor([1.1738, 0.4943, 1.0699]) -- loss 85.30189514160156epoch: 2000 -- Parameters: W: tensor([1.1581, 0.4949, 2.0311]) -- loss 76.05414581298828epoch: 3000 -- Parameters: W: tensor([1.1437, 0.4949, 2.9105]) -- loss 68.31205749511719epoch: 4000 -- Parameters: W: tensor([1.1305, 0.4949, 3.7151]) -- loss 61.83049011230469epoch: 5000 -- Parameters: W: tensor([1.1185, 0.4949, 4.4514]) -- loss 56.4041862487793epoch: 6000 -- Parameters: W: tensor([1.1075, 0.4949, 5.1250]) -- loss 51.86140060424805epoch: 7000 -- Parameters: W: tensor([1.0974, 0.4949, 5.7414]) -- loss 48.058231353759766epoch: 8000 -- Parameters: W: tensor([1.0882, 0.4949, 6.3054]) -- loss 44.8742790222168epoch: 9000 -- Parameters: W: tensor([1.0798, 0.4949, 6.8214]) -- loss 42.20869445800781epoch: 10000 -- Parameters: W: tensor([1.0721, 0.4949, 7.2935]) -- loss 39.97709655761719</code></pre><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x, y, <span class="hljs-string">'o'</span>, label=<span class="hljs-string">'train data'</span>)</span><br><span class="line">plt.plot(x, (x_train.data.matmul(W.data).numpy()), <span class="hljs-string">'-r'</span>, linewidth=<span class="hljs-number">3</span>, label=<span class="hljs-string">'fitted'</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_7_0.png" alt="png"></p><h2 id="2-Cubic-Regression-Model"><a href="#2-Cubic-Regression-Model" class="headerlink" title="2. Cubic Regression Model"></a>2. Cubic Regression Model</h2><script type="math/tex; mode=display">f(x) = w_0 + w_1x + w_2x^2 + w_3x^3</script><h3 id="2-1-Generate-Toy-data"><a href="#2-1-Generate-Toy-data" class="headerlink" title="2.1 Generate Toy data"></a>2.1 Generate Toy data</h3><ul><li>100개의 data 를 생성합니다.</li></ul><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.linspace(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">100</span>)</span><br><span class="line">y = <span class="hljs-number">3</span>*x**<span class="hljs-number">3</span> - <span class="hljs-number">0.2</span> * x ** <span class="hljs-number">2</span> + <span class="hljs-number">0.7</span> * x + <span class="hljs-number">3</span> + <span class="hljs-number">0.5</span> * np.random.rand(len(x))</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x, y, <span class="hljs-string">'o'</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.xlabel(<span class="hljs-string">'x'</span>)</span><br><span class="line">plt.ylabel(<span class="hljs-string">'y'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_11_0.png" alt="png"></p><h3 id="2-2-Define-Model"><a href="#2-2-Define-Model" class="headerlink" title="2.2 Define Model"></a>2.2 Define Model</h3><ul><li>x_train과 y_train 을 만들어줍니다.</li></ul><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x_train = torch.FloatTensor([[xval**<span class="hljs-number">3</span>, xval**<span class="hljs-number">2</span>, xval, <span class="hljs-number">1</span>]<span class="hljs-keyword">for</span> xval <span class="hljs-keyword">in</span> x])</span><br><span class="line">y_train = torch.FloatTensor([y]).view(<span class="hljs-number">100</span>, <span class="hljs-number">-1</span>)</span><br><span class="line">y_train.shape</span><br></pre></td></tr></table></figure><pre><code>torch.Size([100, 1])</code></pre><ul><li>이번에 Model을 nn.Module 추상 클래스를 상속 받아, class 형태로 모델링 해보겠습니다.</li></ul><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CubicModel</span><span class="hljs-params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.linear = nn.Linear(<span class="hljs-number">4</span>, <span class="hljs-number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span></span><br><span class="line">        <span class="hljs-keyword">return</span> self.linear(x)</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = CubicModel()</span><br></pre></td></tr></table></figure><ul><li>train 시킬 때, loss 역시 nn.functional 에 있는 내장 mse loss 를 사용하여 보겠습니다.</li></ul><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="hljs-number">0.001</span>)</span><br><span class="line"></span><br><span class="line">epochs = <span class="hljs-number">15000</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, epochs + <span class="hljs-number">1</span>):</span><br><span class="line">    hypothesis = model(x_train)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># define loss</span></span><br><span class="line">    loss  = F.mse_loss(hypothesis, y_train)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># Backprop &amp; update parameters</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">if</span> epoch % <span class="hljs-number">1500</span> == <span class="hljs-number">0</span>:</span><br><span class="line">        print(<span class="hljs-string">"epoch: &#123;&#125; -- loss &#123;&#125;"</span>.format(epoch, loss.data))</span><br></pre></td></tr></table></figure><pre><code>epoch: 1500 -- loss 0.22306101024150848epoch: 3000 -- loss 0.11560291796922684epoch: 4500 -- loss 0.09848319739103317epoch: 6000 -- loss 0.08879078179597855epoch: 7500 -- loss 0.08104882389307022epoch: 9000 -- loss 0.07452096790075302epoch: 10500 -- loss 0.06889640539884567epoch: 12000 -- loss 0.06398065388202667epoch: 13500 -- loss 0.05964164435863495epoch: 15000 -- loss 0.055785566568374634</code></pre><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x, y, <span class="hljs-string">'o'</span>, label=<span class="hljs-string">'train data'</span>)</span><br><span class="line">plt.plot(x, model(x_train).data.numpy(), <span class="hljs-string">'-r'</span>, linewidth=<span class="hljs-number">3</span>, label=<span class="hljs-string">'fitted'</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_19_0.png" alt="png"></p><h2 id="3-Exponential-Regression-Model"><a href="#3-Exponential-Regression-Model" class="headerlink" title="3. Exponential Regression Model"></a>3. Exponential Regression Model</h2><script type="math/tex; mode=display">f(x) = e^{w_0x}</script><script type="math/tex; mode=display">g(x) = \ln f(x) = w_0x</script><ul><li>Exponential 의 경우, Linear Model 형태를 만들어 주기 위해, log 를 씌워 주워 train 을 시킨후, 다시 exponential 을 양변에 취해주는 형태로 modeling 을 하여야 한다.</li></ul><h3 id="3-1-Generate-Toy-data"><a href="#3-1-Generate-Toy-data" class="headerlink" title="3.1 Generate Toy data"></a>3.1 Generate Toy data</h3><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="hljs-number">20190505</span>)</span><br><span class="line">x = np.linspace(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">50</span>)</span><br><span class="line">y = np.exp(<span class="hljs-number">2</span> * x) + <span class="hljs-number">0.2</span> * (<span class="hljs-number">2</span> * np.random.rand(len(x)) - <span class="hljs-number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x, y, <span class="hljs-string">'o'</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.xlabel(<span class="hljs-string">'x'</span>)</span><br><span class="line">plt.ylabel(<span class="hljs-string">'y'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_23_0.png" alt="png"></p><h3 id="3-2-Define-Model"><a href="#3-2-Define-Model" class="headerlink" title="3.2 Define Model"></a>3.2 Define Model</h3><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x_train = torch.FloatTensor([[xval, <span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> xval <span class="hljs-keyword">in</span> x])</span><br><span class="line">y_train = torch.FloatTensor([np.log(y)]).view(<span class="hljs-number">50</span>, <span class="hljs-number">-1</span>)</span><br><span class="line">y_train.shape</span><br></pre></td></tr></table></figure><pre><code>torch.Size([50, 1])</code></pre><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ExpModel</span><span class="hljs-params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.linear = nn.Linear(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span></span><br><span class="line">        <span class="hljs-keyword">return</span> self.linear(x)</span><br></pre></td></tr></table></figure><ul><li>이번에는 optimize algorithm 중 Adam 을 사용해 보겠습니다.</li><li>Adam 은 adaptive 하게 learning rate 를 조정해 주는 algorithm 입니다.</li></ul><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">model = ExpModel()</span><br><span class="line"></span><br><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line"></span><br><span class="line">epochs = <span class="hljs-number">15000</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, epochs + <span class="hljs-number">1</span>):</span><br><span class="line">    hypothesis = model(x_train)</span><br><span class="line">    </span><br><span class="line">    loss = F.mse_loss(hypothesis, y_train)</span><br><span class="line">    </span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">if</span> epoch % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:</span><br><span class="line">        print(<span class="hljs-string">"epoch: &#123;&#125; -- loss &#123;&#125;"</span>.format(epoch, loss.data))</span><br></pre></td></tr></table></figure><pre><code>epoch: 1000 -- loss 1.4807509183883667epoch: 2000 -- loss 0.6568859815597534epoch: 3000 -- loss 0.2930431365966797epoch: 4000 -- loss 0.1839657723903656epoch: 5000 -- loss 0.1683545857667923epoch: 6000 -- loss 0.16775406897068024epoch: 7000 -- loss 0.16775156557559967epoch: 8000 -- loss 0.16775153577327728epoch: 9000 -- loss 0.16775155067443848epoch: 10000 -- loss 0.16775155067443848epoch: 11000 -- loss 0.16775155067443848epoch: 12000 -- loss 0.16775155067443848epoch: 13000 -- loss 0.16775153577327728epoch: 14000 -- loss 0.1677515208721161epoch: 15000 -- loss 0.1677515208721161</code></pre><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x, y, <span class="hljs-string">'o'</span>, label=<span class="hljs-string">'train data'</span>)</span><br><span class="line">plt.plot(x, np.exp(model(x_train).data.numpy()), <span class="hljs-string">'-r'</span>, linewidth=<span class="hljs-number">3</span>, label=<span class="hljs-string">'fitted'</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_29_0.png" alt="png"></p><h2 id="4-Sine-amp-Cosine-Regression"><a href="#4-Sine-amp-Cosine-Regression" class="headerlink" title="4. Sine &amp; Cosine Regression"></a>4. Sine &amp; Cosine Regression</h2><script type="math/tex; mode=display">f(x) = w_0\cos(\pi x) + w_1\sin(\pi  x)</script><h3 id="4-1-Generate-Toy-data"><a href="#4-1-Generate-Toy-data" class="headerlink" title="4.1 Generate Toy data"></a>4.1 Generate Toy data</h3><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.linspace(<span class="hljs-number">-2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">100</span>)</span><br><span class="line">y = <span class="hljs-number">2</span> * np.cos(np.pi * x) + <span class="hljs-number">1.5</span> * np.sin(np.pi * x) + <span class="hljs-number">2</span> * np.random.rand(len(x)) - <span class="hljs-number">1</span></span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x, y, <span class="hljs-string">'o'</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.xlabel(<span class="hljs-string">'x'</span>)</span><br><span class="line">plt.ylabel(<span class="hljs-string">'y'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_33_0.png" alt="png"></p><h3 id="4-2-Modeling"><a href="#4-2-Modeling" class="headerlink" title="4.2 Modeling"></a>4.2 Modeling</h3><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x_train = torch.FloatTensor([[np.cos(np.pi*xval), np.sin(np.pi*xval), <span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> xval <span class="hljs-keyword">in</span> x])</span><br><span class="line">y_train = torch.FloatTensor(y).view(<span class="hljs-number">100</span>, <span class="hljs-number">-1</span>)</span><br><span class="line">y_train.shape</span><br></pre></td></tr></table></figure><pre><code>torch.Size([100, 1])</code></pre><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SinCosModel</span><span class="hljs-params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.linear = nn.Linear(<span class="hljs-number">3</span>, <span class="hljs-number">1</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span></span><br><span class="line">        <span class="hljs-keyword">return</span> self.linear(x)</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">model = SinCosModel()</span><br><span class="line"></span><br><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line"></span><br><span class="line">epochs = <span class="hljs-number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, epochs + <span class="hljs-number">1</span>):</span><br><span class="line">    hypothesis = model(x_train)</span><br><span class="line">    </span><br><span class="line">    loss = F.mse_loss(hypothesis, y_train)</span><br><span class="line">    </span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">if</span> epoch % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:</span><br><span class="line">        print(<span class="hljs-string">"epoch: &#123;&#125; -- loss &#123;&#125;"</span>.format(epoch, loss.data))</span><br></pre></td></tr></table></figure><pre><code>epoch: 1000 -- loss 1.1333037614822388epoch: 2000 -- loss 0.45972707867622375epoch: 3000 -- loss 0.36056602001190186epoch: 4000 -- loss 0.3566252291202545epoch: 5000 -- loss 0.3566077649593353epoch: 6000 -- loss 0.3566077947616577epoch: 7000 -- loss 0.3566077649593353epoch: 8000 -- loss 0.3566077947616577epoch: 9000 -- loss 0.3566077947616577epoch: 10000 -- loss 0.3566077649593353</code></pre><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x, y, <span class="hljs-string">'o'</span>, label=<span class="hljs-string">'train data'</span>)</span><br><span class="line">plt.plot(x, model(x_train).data.numpy(), <span class="hljs-string">'-r'</span>, linewidth=<span class="hljs-number">3</span>, label=<span class="hljs-string">'fitted'</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_38_0.png" alt="png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Linear-Model-with-Pytorch&quot;&gt;&lt;a href=&quot;#Linear-Model-with-Pytorch&quot; class=&quot;headerlink&quot; title=&quot;Linear Model with Pytorch&quot;&gt;&lt;/a&gt;Linear Mode
      
    
    </summary>
    
      <category term="MachineLearning" scheme="https://emjayahn.github.io/categories/MachineLearning/"/>
    
      <category term="Pytorch" scheme="https://emjayahn.github.io/categories/MachineLearning/Pytorch/"/>
    
    
  </entry>
  
  <entry>
    <title>Linear Regression with Pytorch</title>
    <link href="https://emjayahn.github.io/2019/05/03/Linear-Regression-with-Pytorch/"/>
    <id>https://emjayahn.github.io/2019/05/03/Linear-Regression-with-Pytorch/</id>
    <published>2019-05-03T09:19:59.000Z</published>
    <updated>2019-05-04T14:59:48.212Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Linear-Regression-through-Pytorch"><a href="#Linear-Regression-through-Pytorch" class="headerlink" title="Linear Regression through Pytorch"></a>Linear Regression through Pytorch</h1><ul><li>이번 포스트의 목적은 Linear Model을 Pytorch을 통해 구현해보며, 개인적으로 Pytorch의 사용을 연습하며 적응력을 높여보는 것입니다.</li></ul><h2 id="Import-Library"><a href="#Import-Library" class="headerlink" title="Import Library"></a>Import Library</h2><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> torch</span><br><span class="line"><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="hljs-string">"ignore"</span>)</span><br><span class="line">%config InlineBackend.figure_format = <span class="hljs-string">'retina'</span></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h2 id="Generate-Toy-Data"><a href="#Generate-Toy-Data" class="headerlink" title="Generate Toy Data"></a>Generate Toy Data</h2><p>$ y = \frac{1}{3} x + 5 $ 와 약간의 noise 를 합쳐 100 개의 toy data를 만들겠습니다.</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># Target Function</span></span><br><span class="line">f = <span class="hljs-keyword">lambda</span> x: <span class="hljs-number">1.0</span>/<span class="hljs-number">3.0</span> * x + <span class="hljs-number">5.0</span></span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="hljs-number">-40</span>, <span class="hljs-number">60</span>, <span class="hljs-number">100</span>)</span><br><span class="line">fx = f(x)</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x, fx)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_5_0.png" alt="png"></p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># y_train data with little noise</span></span><br><span class="line">y = fx + <span class="hljs-number">10</span> * np.random.rand(len(x))</span><br><span class="line"></span><br><span class="line">plt.plot(x, y, <span class="hljs-string">'o'</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_6_0.png" alt="png"></p><h2 id="1-Gradient-Descent"><a href="#1-Gradient-Descent" class="headerlink" title="1. Gradient Descent"></a>1. Gradient Descent</h2><ol><li>Model (hypothesis) 를 설정합니다.<br>(여기선, Linear Regression 이므로, $y = Wx + b$ 형태를 사용합니다.)</li><li>Loss Function 을 정의합니다. (여기선, MSE loss 를 사용하겠습니다.)</li><li>gradient 를 계산합니다.<br>(여기선, Gradient Descent 방법으로 optimize 를 할 것이므로, optim.SGD() 를 사용합니다.)</li><li>parameter 를 update 합니다.</li></ol><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_train = torch.FloatTensor(x)</span><br><span class="line">y_train = torch.FloatTensor(y)</span><br><span class="line">print(<span class="hljs-string">"x_train Tensor shape: "</span>, x_train.shape)</span><br><span class="line">print(<span class="hljs-string">"y_train Tensor shape: "</span>, y_train.shape)</span><br></pre></td></tr></table></figure><pre><code>x_train Tensor shape:  torch.Size([100])y_train Tensor shape:  torch.Size([100])</code></pre><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># train code</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># parameter setting &amp; initialize</span></span><br><span class="line">W = torch.zeros(<span class="hljs-number">1</span>, requires_grad=<span class="hljs-keyword">True</span>)</span><br><span class="line">b = torch.zeros(<span class="hljs-number">1</span>, requires_grad=<span class="hljs-keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># optimizer setting</span></span><br><span class="line">optimizer = optim.SGD([W, b], lr=<span class="hljs-number">0.001</span>)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># total epochs</span></span><br><span class="line">epochs = <span class="hljs-number">3000</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, epochs + <span class="hljs-number">1</span>):</span><br><span class="line">    <span class="hljs-comment"># decide model(hypothesis)</span></span><br><span class="line">    model = W * x_train + b</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># loss function -&gt; MSE</span></span><br><span class="line">    loss = torch.mean((model - y_train)**<span class="hljs-number">2</span>)</span><br><span class="line">    </span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 10 epoch 마다 train loss 를 출력합니다.</span></span><br><span class="line">    <span class="hljs-keyword">if</span> epoch % <span class="hljs-number">500</span> == <span class="hljs-number">0</span>:</span><br><span class="line">        print(<span class="hljs-string">"epoch: &#123;&#125; -- Parameters: W: &#123;&#125; b: &#123;&#125; -- loss &#123;&#125;"</span>.format(epoch, W.data, b.data, loss.data))</span><br></pre></td></tr></table></figure><pre><code>epoch: 500 -- Parameters: W: tensor([0.3709]) b: tensor([5.6408]) -- loss 22.728315353393555epoch: 1000 -- Parameters: W: tensor([0.3467]) b: tensor([7.9427]) -- loss 11.399767875671387epoch: 1500 -- Parameters: W: tensor([0.3368]) b: tensor([8.8829]) -- loss 9.51008415222168epoch: 2000 -- Parameters: W: tensor([0.3327]) b: tensor([9.2669]) -- loss 9.194862365722656epoch: 2500 -- Parameters: W: tensor([0.3311]) b: tensor([9.4237]) -- loss 9.142287254333496epoch: 3000 -- Parameters: W: tensor([0.3304]) b: tensor([9.4878]) -- loss 9.133516311645508</code></pre><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x, y, <span class="hljs-string">'o'</span>, label=<span class="hljs-string">"train data"</span>)</span><br><span class="line">plt.plot(x_train.data.numpy(), W.data.numpy()*x + b.data.numpy(), label=<span class="hljs-string">'fitted'</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_10_0.png" alt="png"></p><h2 id="2-Stochastic-Gradient-Descent"><a href="#2-Stochastic-Gradient-Descent" class="headerlink" title="2. Stochastic Gradient Descent"></a>2. Stochastic Gradient Descent</h2><ol><li>Model (hypothesis) Setting</li><li>Loss Function Setting</li><li>최적화 알고리즘 선택</li><li>shuffle train data</li><li>mini-batch 마다 W, b 업데이트</li></ol><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># batch 를 generate 해주는 함수</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">generate_batch</span><span class="hljs-params">(batch_size, x_train, y_train)</span>:</span></span><br><span class="line">    <span class="hljs-keyword">assert</span> len(x_train) == len(y_train)</span><br><span class="line">    result_batches = []</span><br><span class="line">    x_size = len(x_train)</span><br><span class="line">    </span><br><span class="line">    shuffled_id = np.arange(x_size)</span><br><span class="line">    np.random.shuffle(shuffled_id)</span><br><span class="line">    shuffled_x_train = x_train[shuffled_id]</span><br><span class="line">    shuffled_y_train = y_train[shuffled_id]</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">for</span> start_idx <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, x_size, batch_size):</span><br><span class="line">        end_idx = start_idx + batch_size</span><br><span class="line">        batch = [shuffled_x_train[start_idx:end_idx], shuffled_y_train[start_idx:end_idx]]</span><br><span class="line">        result_batches.append(batch)</span><br><span class="line">    <span class="hljs-keyword">return</span> result_batches</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># train</span></span><br><span class="line">W = torch.zeros(<span class="hljs-number">1</span>, requires_grad=<span class="hljs-keyword">True</span>)</span><br><span class="line">b = torch.zeros(<span class="hljs-number">1</span>, requires_grad=<span class="hljs-keyword">True</span>)</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD([W, b], lr=<span class="hljs-number">0.001</span>)</span><br><span class="line"></span><br><span class="line">epochs = <span class="hljs-number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, epochs + <span class="hljs-number">1</span>):</span><br><span class="line">    <span class="hljs-keyword">for</span> x_batch, y_batch <span class="hljs-keyword">in</span> generate_batch(<span class="hljs-number">10</span>, x_train, y_train):</span><br><span class="line">        model = W * x_batch + b</span><br><span class="line">        loss = torch.mean((model - y_batch)**<span class="hljs-number">2</span>)</span><br><span class="line">        </span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">    <span class="hljs-keyword">if</span> epoch % <span class="hljs-number">500</span> == <span class="hljs-number">0</span>:</span><br><span class="line">        print(<span class="hljs-string">"epoch: &#123;&#125; -- Parameters: W: &#123;&#125; b: &#123;&#125; -- loss &#123;&#125;"</span>.format(epoch, W.data, b.data, loss.data))</span><br></pre></td></tr></table></figure><pre><code>epoch: 500 -- Parameters: W: tensor([0.0890]) b: tensor([9.5399]) -- loss 162.1055450439453epoch: 1000 -- Parameters: W: tensor([0.3672]) b: tensor([9.5366]) -- loss 12.424881935119629epoch: 1500 -- Parameters: W: tensor([0.3560]) b: tensor([9.5097]) -- loss 7.826609134674072epoch: 2000 -- Parameters: W: tensor([0.3375]) b: tensor([9.5556]) -- loss 13.15934944152832epoch: 2500 -- Parameters: W: tensor([0.2462]) b: tensor([9.5157]) -- loss 11.582895278930664epoch: 3000 -- Parameters: W: tensor([0.3097]) b: tensor([9.5111]) -- loss 9.991677284240723epoch: 3500 -- Parameters: W: tensor([0.2497]) b: tensor([9.5532]) -- loss 20.481367111206055epoch: 4000 -- Parameters: W: tensor([0.4388]) b: tensor([9.5390]) -- loss 20.827198028564453epoch: 4500 -- Parameters: W: tensor([0.1080]) b: tensor([9.4959]) -- loss 140.0277862548828epoch: 5000 -- Parameters: W: tensor([0.3188]) b: tensor([9.4829]) -- loss 6.635367393493652epoch: 5500 -- Parameters: W: tensor([0.2553]) b: tensor([9.5017]) -- loss 25.45773696899414epoch: 6000 -- Parameters: W: tensor([0.2490]) b: tensor([9.5489]) -- loss 9.580666542053223epoch: 6500 -- Parameters: W: tensor([0.3189]) b: tensor([9.5347]) -- loss 12.585128784179688epoch: 7000 -- Parameters: W: tensor([0.3026]) b: tensor([9.4874]) -- loss 8.298829078674316epoch: 7500 -- Parameters: W: tensor([0.3507]) b: tensor([9.6815]) -- loss 13.348054885864258epoch: 8000 -- Parameters: W: tensor([0.1423]) b: tensor([9.5220]) -- loss 32.567440032958984epoch: 8500 -- Parameters: W: tensor([0.7147]) b: tensor([9.5182]) -- loss 75.97190856933594epoch: 9000 -- Parameters: W: tensor([0.5170]) b: tensor([9.5289]) -- loss 39.07848358154297epoch: 9500 -- Parameters: W: tensor([0.3748]) b: tensor([9.5590]) -- loss 10.358983993530273epoch: 10000 -- Parameters: W: tensor([0.2958]) b: tensor([9.6088]) -- loss 7.410649299621582</code></pre><ul><li>Stochasitic 하게 loss의 gradient 를 계산하여, parameter update를 하므로, loss 가 굉장히 oscilation 이 나타나며 감소하는 것을 볼 수 있다.</li></ul><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x, y, <span class="hljs-string">'o'</span>, label=<span class="hljs-string">"train data"</span>)</span><br><span class="line">plt.plot(x_train.data.numpy(), W.data.numpy()*x + b.data.numpy(), label=<span class="hljs-string">'fitted'</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_15_0.png" alt="png"></p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Linear-Regression-through-Pytorch&quot;&gt;&lt;a href=&quot;#Linear-Regression-through-Pytorch&quot; class=&quot;headerlink&quot; title=&quot;Linear Regression through 
      
    
    </summary>
    
      <category term="MachineLearning" scheme="https://emjayahn.github.io/categories/MachineLearning/"/>
    
      <category term="Pytorch" scheme="https://emjayahn.github.io/categories/MachineLearning/Pytorch/"/>
    
    
  </entry>
  
  <entry>
    <title>[Lecture] 딥러닝을 이용한 자연어 처리 Section A, B</title>
    <link href="https://emjayahn.github.io/2019/05/03/Lecture-%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%9E%90%EC%97%B0%EC%96%B4-%EC%B2%98%EB%A6%AC-Section-A-B/"/>
    <id>https://emjayahn.github.io/2019/05/03/Lecture-딥러닝을-이용한-자연어-처리-Section-A-B/</id>
    <published>2019-05-03T04:24:28.000Z</published>
    <updated>2019-05-03T04:25:33.349Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Section-A-B-Summary"><a href="#Section-A-B-Summary" class="headerlink" title="Section A, B - Summary"></a>Section A, B - Summary</h1><p>이 글은 edwith(<a href="https://www.edwith.org/" target="_blank" rel="noopener">https://www.edwith.org/</a>)의 조경현 교수님의 딥러닝을 이용한 자연어 처리 (<a href="https://www.edwith.org/deepnlp/joinLectures/17363" target="_blank" rel="noopener">https://www.edwith.org/deepnlp/joinLectures/17363</a>)강의를 듣고 정리한 글입니다.</p><h2 id="Section-A-Introduction"><a href="#Section-A-Introduction" class="headerlink" title="Section A. Introduction"></a>Section A. Introduction</h2><ul><li>알고리즘의 정의 : 문제를 해결하기 위한 instruction 의 sequence</li><li><p>Machine Learning Algorithm</p><ol><li><p>문제 정의 (optional) : 문제를 specific 하게 정의 하는 것 자체가 쉽지 않다.</p><p> ex) 얼굴을 detection 할 때, 어떤 범위까지 얼굴이라고 정의할 것인지</p></li><li><p>Example들이 주어진다 → 데이터들</p></li><li>문제를 해결 할 수 있는 Train된 Machine Learning Model</li></ol></li></ul><h2 id="Section-B-Basic-Machine-Learning-Supervised-Learning"><a href="#Section-B-Basic-Machine-Learning-Supervised-Learning" class="headerlink" title="Section B. Basic Machine Learning: Supervised Learning"></a>Section B. Basic Machine Learning: Supervised Learning</h2><h3 id="0-Supervised-Learning"><a href="#0-Supervised-Learning" class="headerlink" title="0. Supervised Learning"></a>0. Supervised Learning</h3><ul><li><p>제공되는 것들:</p><ol><li><p>N 개의 pair 로 된 training set</p><script type="math/tex; mode=display">D = {(x_1, y_1), ..., (x_N, y_N)}</script></li><li><p>Data 별 loss function</p><ul><li>Loss function 은 필요에 따라서 우리가 디자인 해야 할 때도 있다.</li></ul><script type="math/tex; mode=display">l(M(x), y) \geq 0</script></li><li><p>Evaluation sets: Validation set과 test set</p></li></ol><ul><li>기존에 보지 못한 dataset 에도 trained model 이 잘 작동 하는지 확인하는 것이 필수</li></ul></li><li>우리가 결정해야 하는 것들:<ol><li>Hypothesis sets: H1, H2 … :<ul><li>모델, hyper parameter들이 다른 모델, 여러가지 실험 해보고 싶은 것들이 될 수 있다.</li><li>가설을 잘 설정하는 것이 최종의 모델을 결정하는데 중요한 기초 작업</li></ul></li><li>Optimization Algorithm<ul><li>어떤 방법론으로 최적화를 진행 할지 역시 매우 중요한 문제</li></ul></li></ol></li><li>결국, 우리가 해야 하는 것은<ol><li>주어진 Training set 에서, hypothesis set 안의 각 Hm 마다 가장 좋은 모델을 찾는다.</li><li>Trained Hm 중에서, Validation set 에서 가장 좋은 한 가지 모델을 결정한다.</li><li>Reporting 을 위해 test set 에서 얼마나 좋은 성능을 나타내는지 확인한다.</li></ol></li></ul><h3 id="Three-points-to-Consider-both-in-research-and-in-practice"><a href="#Three-points-to-Consider-both-in-research-and-in-practice" class="headerlink" title="[Three points to Consider both in research and in practice]"></a>[Three points to Consider both in research and in practice]</h3><h3 id="1-어떻게-Hypothesis-set-을-설정하는가"><a href="#1-어떻게-Hypothesis-set-을-설정하는가" class="headerlink" title="1. 어떻게 Hypothesis set 을 설정하는가?"></a>1. 어떻게 Hypothesis set 을 설정하는가?</h3><ul><li><p>Hypothesis Set 자체가 infinite 하다는 문제</p><ul><li>Neural Network 에서 국한되서 보자면, 어떤 Network Architecture 를 사용하여 모델을 구성할 것인지,  각 모델마다 hyper parameter 를 어떻게 설정할 것인지 등 hypothesis set 이 매우 다양하다.</li><li>이 중, 좋은 한가지 모델을 한가지 찾는 방법이 어렵다.</li><li>강의 표현 중 이를 찾는 것은 Science ——— Magic 사이에 어느 한 점인, 거의 Art 에 가깝다고 하셔서 매우 웃겼다. 개인적으로 이 부분이 가장 공부하면서도 어렵고, 그 모델을 찾는 것이 매우 추상적인 느낌이다. 그리고 개인적으로 진행하는 프로젝트에서 과연 내가 찾은 모델보다 더 좋은 성능을 가지는 모델혹은 파라미터는 없을까(무조건 있을 것인데..라고 생각하는 경우가 훨씬 많지만..)라고 생각하며 분석과 모델링의 열정을 높이곤한다.</li></ul></li><li><p>Network Architectures</p><ul><li>Neural Network 는 Directed Acyclic Graph이다!!</li><li>Inference : Forward Computaion 만으로, 쉽게 trained neural network를 사용할 수 있다.</li><li>이를 구성하는데 있어, high-level 로 abstraction 된 라이브러리를 oop , functional programming 을 활용해 쉽게 구현할 수 있다. (pytorch, tensorflow…)</li></ul></li></ul><h3 id="2-Loss-Function"><a href="#2-Loss-Function" class="headerlink" title="2.  Loss Function"></a>2.  Loss Function</h3><ul><li><p>관점의 이동:</p><ul><li><p>어떤 주어진 data x 에 대하여 y 는 무엇일까? 를 생각하는 모델이 아니라,</p><script type="math/tex; mode=display">f_\theta(x) = ?</script></li><li><p>주어진 x 에 대해 y가 어떤 case 혹은 값일 확률로 생각한다.</p><script type="math/tex; mode=display">p(y=y'|x) = ?</script></li></ul></li><li><p>Distribution based loss functions</p><ul><li>Binary Classification : Bernoulli distribution → Sigmoid</li><li>Multiclass Classification : Categorical distribution → Softmax</li><li>Linear Regression : Gaussian distribution</li><li>Multimodal linear Regression : Mixture of Gaussians</li></ul></li><li>결국 Loss Function 은 다음과 같이 표현될 수 있다. Maximize logp</li></ul><script type="math/tex; mode=display">argmax_\theta \sum_{n=1}^{N} logp_\theta(y_n|x_n)</script><ul><li>= minimize L(theta):</li></ul><script type="math/tex; mode=display">L(\theta) = \sum_{n=1}^{N} l(M_\theta(x_n), y_n)=  -\sum_{n=1}^{N} logp_\theta(y_n|x_n)</script><h3 id="3-Optimization"><a href="#3-Optimization" class="headerlink" title="3. Optimization"></a>3. Optimization</h3><ul><li>Optimization 방법<ul><li>Optimization 방법에는 GD, SGD, Newton Method 등 다양한 방법이 있지만, Nerual Network 에서는 Gradient Descent 방법을 위주로 사용한다. 이 강의에서는 다루지 않았지만, Gradient Descent  방법 외의 다른 알고리즘들은 전제되는 가정들이 많고, Nerual Network 의 고차원에서는 그 가정을 만족하기가 쉽지 않다. (예를 들어, Newton Method 에서의 Hessian 행렬이 구해지기 위한 가정, computation 양 또한 매우 많다.) 이런 이유에서 Gradient Descent 방법을 사용한다.</li></ul></li><li>Backward Computation : Backpropagation<ul><li>Loss function 의 gradient 를 구하는 방법은 쉽지 않다.</li><li>Neural Network는 Automatic differentiation (Autograd) 를 사용하여, weight 과 bias term 에 대해 쉽게(?) gradient 값을 구할 수 있다. library 덕분에 우리가 loss function 의 gradient 를 직접 구하지 않아도 된다.</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Section-A-B-Summary&quot;&gt;&lt;a href=&quot;#Section-A-B-Summary&quot; class=&quot;headerlink&quot; title=&quot;Section A, B - Summary&quot;&gt;&lt;/a&gt;Section A, B - Summary&lt;/h1
      
    
    </summary>
    
      <category term="Lecture" scheme="https://emjayahn.github.io/categories/Lecture/"/>
    
      <category term="딥러닝을 이용한 자연어 처리" scheme="https://emjayahn.github.io/categories/Lecture/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%9E%90%EC%97%B0%EC%96%B4-%EC%B2%98%EB%A6%AC/"/>
    
    
  </entry>
  
  <entry>
    <title>Provision</title>
    <link href="https://emjayahn.github.io/2019/01/10/Provision/"/>
    <id>https://emjayahn.github.io/2019/01/10/Provision/</id>
    <published>2019-01-10T14:34:05.000Z</published>
    <updated>2019-01-10T14:36:51.338Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Server-Provisioning-amp-Terraform"><a href="#Server-Provisioning-amp-Terraform" class="headerlink" title="Server Provisioning &amp; Terraform"></a>Server Provisioning &amp; Terraform</h1><ul><li>Provisioning 이란,  한정된 자원을 최적의 효율을 위해 제공하는 기술적 개념을 말한다. 유저의 요청에 맞게 자원을 미리 세팅해두고, 유저의 요청에 따라 준비된 자원들을 목적과 효율에 맞게 제공하는 개념이다. 특정 분야에서 한정되어 사용하는 개념이 아니라 다양한 분야에서 응용되어지는 주제이다.  (IT 분야만으로 한정되지도 않는다) IT 분야의 Provisioning의 예시로는, Server Provisioning, Storage Provisioning, Telecommunication Provisioning 등이 있다.</li><li>여기서는 <strong>Terraform</strong> 을 활용한 AWS 서버 프로비져닝에 관해 다룬다.</li></ul><h2 id="AWS-EC2"><a href="#AWS-EC2" class="headerlink" title="## AWS EC2"></a>## AWS EC2</h2><ul><li>AWS EC2를 활용하기 위해서는 3가지의 기본적인 세팅이 필요하다.<ol><li>키페어 (Key pair)</li><li>보안그룹 (Security Group)</li><li>인스턴스 (Instance)</li></ol></li><li>이 세가지를 Terraform 을 활용해 생성하는 코드를 정리한다.</li></ul><h3 id="1-키페어-생성-Key-pair"><a href="#1-키페어-생성-Key-pair" class="headerlink" title="1. 키페어 생성 (Key pair)"></a>1. 키페어 생성 (Key pair)</h3><h4 id="1-1-Key-만들기"><a href="#1-1-Key-만들기" class="headerlink" title="1-1. Key 만들기"></a>1-1. Key 만들기</h4><pre><code>- 자신의 email로 ssh key 를 생성하여, key_name 이름으로 .ssh 폴더에 저장</code></pre><p><code>ssh-keygen -t rsa -b 4096 -C “email” -f “$HOME/.ssh/key_name” -N “”</code></p><pre><code>- 이렇게 생성된 key 는 /key_name/ 과 /key_name.pub/로 private key 와 public key가 생성된다.</code></pre><h4 id="1-2-Key-pair-생성"><a href="#1-2-Key-pair-생성" class="headerlink" title="1-2. Key pair 생성"></a>1-2. Key pair 생성</h4><pre><code>- `main.tf`  파일 생성</code></pre><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">`provider “aws” &#123;</span><br><span class="line"># 이 region 은 seoul 을 의미한다</span><br><span class="line">region = “ap-northeast-2”</span><br><span class="line">&#125;</span><br><span class="line">Resource “aws_key_pair” “resource_name” &#123;</span><br><span class="line"># keygen 으로 생성한 key_name</span><br><span class="line">key_name = &quot;key_name&quot;</span><br><span class="line">public_key = &quot;$&#123;file(&quot;~/.ssh/key_name.pub&quot;)&#125;&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><pre><code>- apply 를 실행해, aws 키페어를 생성</code></pre><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ terraform init</span><br><span class="line">$ terraform plan</span><br><span class="line">$ terraform apply</span><br></pre></td></tr></table></figure><pre><code>- destroy 를 실행해, aws 키페어를 삭제</code></pre><p><code>$ terraform destroy</code></p><h3 id="2-보안그룹-생성-Security-Group"><a href="#2-보안그룹-생성-Security-Group" class="headerlink" title="2. 보안그룹 생성 (Security Group)"></a>2. 보안그룹 생성 (Security Group)</h3><pre><code>- 보안그룹은 생성될 인스턴스의 정책을 설정하는 부분이다. 가장 대표적인 기능은 인바운드와 아웃바운드 port 를 설정할 수 있다.- 필요에 따라 port number 를 열어주면 된다.- `ingress` 는 인바운드, `egress` 는 아웃바운드 태그이다.- `from_port` 와 `to_port` 는 말 그대로 from 부터 to 까지의 번호를 지정한다.- 대표적인 포트번호에 관한 설명    - 22 : ssh 접속을 위한 포트    - 80 : http의 기본포트    - 8888 : Jupiter notebook 사용을 위한  포트    - 27017 : MongoDB 사용을 위한 포트    - 3306 : MySQL 사용을 위한 포트( /여기서는 DB의 경우, 다른 서버를 두고 사용하고 있으므로, 열어주지 않는다/ )- `main.tf`  파일 생성</code></pre><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">provider &quot;aws&quot; &#123;</span><br><span class="line">    region = &quot;ap-northeast-2&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">resource &quot;aws_security_group&quot; &quot;resource_name&quot; &#123;</span><br><span class="line">name = &quot;보안그룹 이름&quot;</span><br><span class="line">description = &quot;보안그룹의 설명&quot;</span><br><span class="line">ingress &#123;</span><br><span class="line">from_port = 22</span><br><span class="line">to_port = 22</span><br><span class="line">protocol = &quot;tcp&quot;</span><br><span class="line">cidr_blocks = [&quot;0.0.0.0/0&quot;]</span><br><span class="line">&#125;</span><br><span class="line">ingress &#123;</span><br><span class="line">from_port = 80</span><br><span class="line">to_port = 80</span><br><span class="line">protocol = &quot;tcp&quot;</span><br><span class="line">cidr_blocks = [&quot;0.0.0.0/0&quot;]</span><br><span class="line">&#125;</span><br><span class="line">ingress &#123;</span><br><span class="line">from_port = 8888</span><br><span class="line">to_port = 8888</span><br><span class="line">protocol = &quot;tcp&quot;</span><br><span class="line">cidr_blocks = [&quot;0.0.0.0/0&quot;]</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>마찬가지로, <code>init</code>, <code>plan</code>, <code>apply</code> 를 활용하여 보안그룹을 생성하고, <code>destroy</code> 로 제거한다.</li></ul><h3 id="3-인스턴스-생성"><a href="#3-인스턴스-생성" class="headerlink" title="3. 인스턴스 생성"></a>3. 인스턴스 생성</h3><pre><code>- 대망의 인스턴스 생성!- 여기선, EC2 중, linux ubuntu 18.04 버젼 을 활용한다. AWS 내에서 다른 종류의 인스턴스를 사용할 경우, ami 를 다른 값으로 사용하면 된다.- 또한, 다양한 인스턴스 유형중, t2.nano를 사용한다. 인스턴스 유형도 필요에 따라 다르게 설정하면 된다.- `main.tf`  파일 생성</code></pre><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">provider &quot;aws&quot; &#123;</span><br><span class="line">region = &quot;ap-northeast-2&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data &quot;aws_security_group&quot; &quot;resource1&quot; &#123;</span><br><span class="line">     name = &quot;security_group_name&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">resource &quot;aws_instance&quot; &quot;resource2&quot; &#123;</span><br><span class="line">ami = &quot;ami-06e7b9c5e0c4dd014&quot;</span><br><span class="line">instance_type = &quot;t2.nano&quot;</span><br><span class="line">key_name = &quot;key_name&quot;</span><br><span class="line">vpc_security_group_ids = [</span><br><span class="line"> &quot;$&#123;data.aws_security_group.resource1.id&#125;&quot;</span><br><span class="line">]</span><br><span class="line">tags &#123;</span><br><span class="line">Name = &quot;dss_instance&quot;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><pre><code>- `init`, `plan`, `apply` 를 활용하여  인스턴스를 생성하고, `destroy` 로 제거한다.</code></pre><h3 id="4-생성된-인스턴스-확인"><a href="#4-생성된-인스턴스-확인" class="headerlink" title="4. 생성된 인스턴스 확인"></a>4. 생성된 인스턴스 확인</h3><p><code>ssh -I ~/.ssh/key_name ubuntu@외부ip</code> 로 생성된 인스턴스를 확인하고, 활용할 수 있다.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Server-Provisioning-amp-Terraform&quot;&gt;&lt;a href=&quot;#Server-Provisioning-amp-Terraform&quot; class=&quot;headerlink&quot; title=&quot;Server Provisioning &amp;amp; 
      
    
    </summary>
    
      <category term="wiki" scheme="https://emjayahn.github.io/categories/wiki/"/>
    
      <category term="Provision" scheme="https://emjayahn.github.io/categories/wiki/Provision/"/>
    
    
  </entry>
  
  <entry>
    <title>190109-TodayWhatILearned</title>
    <link href="https://emjayahn.github.io/2019/01/09/190109-TodayWhatILearned/"/>
    <id>https://emjayahn.github.io/2019/01/09/190109-TodayWhatILearned/</id>
    <published>2019-01-09T14:58:09.000Z</published>
    <updated>2019-06-25T07:05:10.983Z</updated>
    
    <content type="html"><![CDATA[<h1 id="190109-TWIL"><a href="#190109-TWIL" class="headerlink" title="190109 TWIL"></a>190109 TWIL</h1><hr><h2 id="오늘-한-일은-무엇인가"><a href="#오늘-한-일은-무엇인가" class="headerlink" title="오늘 한 일은 무엇인가"></a>오늘 한 일은 무엇인가</h2><h2 id="1-BLOG-RENEWAL"><a href="#1-BLOG-RENEWAL" class="headerlink" title="1. BLOG RENEWAL"></a>1. BLOG RENEWAL</h2><h2 id="내일-할-일은-무엇인가"><a href="#내일-할-일은-무엇인가" class="headerlink" title="내일 할 일은 무엇인가"></a>내일 할 일은 무엇인가</h2><ol><li>Graph모형 공부</li><li>LinearAlgebra 1강, 2강<br><br></li></ol><hr><h2 id="무엇을-느꼈는가"><a href="#무엇을-느꼈는가" class="headerlink" title="무엇을 느꼈는가"></a>무엇을 느꼈는가</h2><ul><li>새해를 맞아 블로그를 새 테마로 바꾸었다. 기존에 hueman theme 에 익숙해져 있어서, 새로운 테마의 기능을<br>수정하고, 전처럼 편해지려면 또 적응의 시간이 필요할 것 같다. 블로그의 테마는 작년부터 글의 양이 늘어나면 늘어날수록<br>그 욕구가 더 심해 졌다. 특정 카테고리에서 글이 누적되가면서, 어떤 글들이 담겨있는지 제목을 통해 직관적으로<br>보고싶었다. hueman 은 글마다 썸네일들이 있고, 글의 순서가 조금 불편하게 배치되어 있다. 시리즈성 글들을 올린다거나,<br>주제가 1, 2, 3 등으로 나뉘는 글들이 있을 때, 글 제목으로 연속성을 보기가 힘들었다.</li><li>위의 이유로 선택한 이번 테마는 내가 중점적으로 생각한 부분을 조금이나마 개선할 수 있는 것 같다. 틈틈히<br>새로운 테마의 세팅도 마쳐야겠다.</li></ul><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;190109-TWIL&quot;&gt;&lt;a href=&quot;#190109-TWIL&quot; class=&quot;headerlink&quot; title=&quot;190109 TWIL&quot;&gt;&lt;/a&gt;190109 TWIL&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;오늘-한-일은-무엇인가&quot;&gt;&lt;a href=&quot;#
      
    
    </summary>
    
      <category term="Diary" scheme="https://emjayahn.github.io/categories/Diary/"/>
    
    
  </entry>
  
  <entry>
    <title>REBOOT</title>
    <link href="https://emjayahn.github.io/2019/01/08/190108-TodayWhatILearned/"/>
    <id>https://emjayahn.github.io/2019/01/08/190108-TodayWhatILearned/</id>
    <published>2019-01-08T02:35:35.000Z</published>
    <updated>2019-01-08T11:41:04.351Z</updated>
    
    <content type="html"><![CDATA[<p><strong> REBOOT </strong></p><ul><li>Text Classification Project 를 한다는 핑계로 그간 TodayWhatILearned의 작성을 하지 못했다.<br>프로젝트를 하는 동안은 매일 어떤 것을 공부할 계획이고, 어떤 공부를 했는지 남길 만한 내용이 없었던 것도 사실이다.<br>프로젝트 동안 미뤄뒀던 공부들, 보고싶었던 주제들을 이제 다시 새로운 마음가짐을 가지고 시작할 것이다.<br>새해가 밝은 만큼 블로그를 만들기 시작하면서 다짐했던 초심을 상기하자.</li></ul><h2 id="To-Do-List"><a href="#To-Do-List" class="headerlink" title="To-Do-List"></a>To-Do-List</h2><ol><li>Graph모형, 네트워크 추론 공부 (수식) - 새로운 패키지, 코드 정리하면서 공부</li><li>LinearAlgebra 1강, 2강 다시 시작</li></ol><hr><h2 id="오늘-한-일은-무엇인가"><a href="#오늘-한-일은-무엇인가" class="headerlink" title="오늘 한 일은 무엇인가"></a>오늘 한 일은 무엇인가</h2><ol><li>Graph모형 공부</li><li>LinearAlgebra 1강, 2강<br><br></li></ol><hr><h2 id="내일-할-일은-무엇인가"><a href="#내일-할-일은-무엇인가" class="headerlink" title="내일 할 일은 무엇인가"></a>내일 할 일은 무엇인가</h2><h2 id="1-네트워크-추론-공부-수식-위주로-공부"><a href="#1-네트워크-추론-공부-수식-위주로-공부" class="headerlink" title="1. 네트워크 추론 공부(수식 위주로 공부)"></a>1. 네트워크 추론 공부(수식 위주로 공부)</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt; REBOOT &lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Text Classification Project 를 한다는 핑계로 그간 TodayWhatILearned의 작성을 하지 못했다.&lt;br&gt;프로젝트를 하는 동안은 매일 어떤 것을 공
      
    
    </summary>
    
      <category term="Diary" scheme="https://emjayahn.github.io/categories/Diary/"/>
    
    
  </entry>
  
  <entry>
    <title>181225-TodayWhatILearned</title>
    <link href="https://emjayahn.github.io/2018/12/26/181225-TodayWhatILearned/"/>
    <id>https://emjayahn.github.io/2018/12/26/181225-TodayWhatILearned/</id>
    <published>2018-12-25T16:15:58.000Z</published>
    <updated>2018-12-25T16:32:57.028Z</updated>
    
    <content type="html"><![CDATA[<h1 id="181225-TWIL"><a href="#181225-TWIL" class="headerlink" title="181225 TWIL"></a>181225 TWIL</h1><hr><h2 id="오늘-한-일은-무엇인가"><a href="#오늘-한-일은-무엇인가" class="headerlink" title="오늘 한 일은 무엇인가"></a>오늘 한 일은 무엇인가</h2><h2 id="1-Project-Text-Preprocessing"><a href="#1-Project-Text-Preprocessing" class="headerlink" title="1. (Project) Text Preprocessing"></a>1. (Project) Text Preprocessing</h2><h2 id="내일-할-일은-무엇인가"><a href="#내일-할-일은-무엇인가" class="headerlink" title="내일 할 일은 무엇인가"></a>내일 할 일은 무엇인가</h2><ol><li>(Project) Project 모임</li><li>Linear Algebra 강의 2강, 3강 듣기<br><br></li></ol><hr><h2 id="무엇을-느꼈는가"><a href="#무엇을-느꼈는가" class="headerlink" title="무엇을 느꼈는가"></a>무엇을 느꼈는가</h2><ul><li>모든 전처리가 그렇겠지만, 텍스트 데이터의 전처리는 유독 할게 많다. 실제 사람이 사용하는 언어 데이터이다 보니,<br>예외사항들이 많고 모델 성능에 이 전처리들이 큰 영향을 미친다고 하기에, 열심히 전처리를 하고 있다.</li><li>오늘은 embedding 데이터를 활용해 줄임말들 (I’d, We’re 등) 늘려주는 작업을(I would, We are 등) 해줬다.<br>기존에 짰던 영어이냐 아니냐를 분류하려고 만든 알고리즘의 성능이 위 작업을 통해 좀 더 좋아질 것이라 예상된다. 또한<br>오늘 한 작업이 main modeling 을 하기 위해 진행할 Tokenizing 에도 좋은 영향을 줄 것이다. Stopwords 들을<br>빼거나 더할 때도, What’s 보다는 What is 로 늘려주었을 때, 훨씬더 세밀해 질 것이다.</li><li>내일은 spelling 체크, 띄어쓰기 체크해서 올바르게 고쳐주는 작업을 해야한다. 그리고, Baseline 모델을<br>잡기 위해 본격적인 modeling 에 들어가야한다.</li></ul><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;181225-TWIL&quot;&gt;&lt;a href=&quot;#181225-TWIL&quot; class=&quot;headerlink&quot; title=&quot;181225 TWIL&quot;&gt;&lt;/a&gt;181225 TWIL&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;오늘-한-일은-무엇인가&quot;&gt;&lt;a href=&quot;#
      
    
    </summary>
    
      <category term="Diary" scheme="https://emjayahn.github.io/categories/Diary/"/>
    
    
  </entry>
  
  <entry>
    <title>181220-TodayWhatILearned</title>
    <link href="https://emjayahn.github.io/2018/12/20/181220-TodayWhatILearned/"/>
    <id>https://emjayahn.github.io/2018/12/20/181220-TodayWhatILearned/</id>
    <published>2018-12-20T14:34:46.000Z</published>
    <updated>2018-12-20T14:40:01.203Z</updated>
    
    <content type="html"><![CDATA[<h1 id="181220-TWIL"><a href="#181220-TWIL" class="headerlink" title="181220 TWIL"></a>181220 TWIL</h1><hr><h2 id="오늘-한-일은-무엇인가"><a href="#오늘-한-일은-무엇인가" class="headerlink" title="오늘 한 일은 무엇인가"></a>오늘 한 일은 무엇인가</h2><ol><li>(Project) Classification Project 모임</li><li>딥러닝 엔지니어 현업자 특강</li><li>Celery 복습</li><li>간단한 알고리즘 문제 풀기</li><li>Linear Algebra(Gilbert) 1강</li></ol><hr><h2 id="내일-할-일은-무엇인가"><a href="#내일-할-일은-무엇인가" class="headerlink" title="내일 할 일은 무엇인가"></a>내일 할 일은 무엇인가</h2><ol><li>Linear Algebra(Gilbert) 2강</li><li>(Project) Classification Project</li><li>Classification 개념 다시 보기<br><br></li></ol><hr><h2 id="무엇을-느꼈는가"><a href="#무엇을-느꼈는가" class="headerlink" title="무엇을 느꼈는가"></a>무엇을 느꼈는가</h2><h2 id="즐기자"><a href="#즐기자" class="headerlink" title="- 즐기자"></a>- 즐기자</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;181220-TWIL&quot;&gt;&lt;a href=&quot;#181220-TWIL&quot; class=&quot;headerlink&quot; title=&quot;181220 TWIL&quot;&gt;&lt;/a&gt;181220 TWIL&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;오늘-한-일은-무엇인가&quot;&gt;&lt;a href=&quot;#
      
    
    </summary>
    
      <category term="Diary" scheme="https://emjayahn.github.io/categories/Diary/"/>
    
    
  </entry>
  
  <entry>
    <title>181216-TodayWhatILearned</title>
    <link href="https://emjayahn.github.io/2018/12/16/181216-TodayWhatILearned/"/>
    <id>https://emjayahn.github.io/2018/12/16/181216-TodayWhatILearned/</id>
    <published>2018-12-16T13:01:09.000Z</published>
    <updated>2018-12-16T13:23:02.493Z</updated>
    
    <content type="html"><![CDATA[<h1 id="181216-TWIL"><a href="#181216-TWIL" class="headerlink" title="181216 TWIL"></a>181216 TWIL</h1><hr><h2 id="오늘-한-일은-무엇인가"><a href="#오늘-한-일은-무엇인가" class="headerlink" title="오늘 한 일은 무엇인가"></a>오늘 한 일은 무엇인가</h2><ol><li>DataThon 발표</li><li>Perceptron 공부</li><li>(Project) Quara Dataset EDA<ul><li>question_text 에서 vectorize 하기전에 특징값들을 뽑아내기</li><li>나이브 베이지안 돌려보기</li></ul></li><li>(Study) 스터디때 나눌 WebApplication 의 구조, MVC model 나누기</li></ol><hr><h2 id="내일-할-일은-무엇인가"><a href="#내일-할-일은-무엇인가" class="headerlink" title="내일 할 일은 무엇인가"></a>내일 할 일은 무엇인가</h2><ol><li>(Study) 스터디원 블로그 개설, Flask에 비유한 WebApplication, MVC model 공부하기</li><li>(Stydy) PROJECTmini WebApplication 계획 세우기</li><li>(Project) EDA 짬짬히 계속하기</li><li>SVM 공부<br><br></li></ol><hr><h2 id="무엇을-느꼈는가"><a href="#무엇을-느꼈는가" class="headerlink" title="무엇을 느꼈는가"></a>무엇을 느꼈는가</h2><ul><li>Datathon에서 분석했던 내용을 발표하는 시간을 가졌다. 발표를 하면서 부족하다고 생각했던 점과 comment 를<br>잊기 전에 정리해본다.<ol><li>후기 및 생각과 느낌<ol><li>프레젠테이션 능력이 부족하다.<ul><li>긴장, 생각의 흐름을 말로 표현하는 것이 부족했다. 나름대로 이야기 할 것을 리스트업해갔지만, 잘 눈에 들어오지 않았다.</li><li>스크립트를 다 작성해가는 것이 좋은 것일까?</li></ul></li><li>프레젠테이션 혹은 데이터를 모르는 사람도 읽을 수 있는 마크다운 정리가 부족했다.<ul><li>데이터톤 당시 시간에 쫓기는 것도 있었고, 데이터를 분석하고 코드를 작성하면서 <code>나중에 하면 되겠지</code> 라고 생각했다.</li><li>결과는 제대로 마무리와 정리를 하지 못한 채로 제출했고, 이는 발표할 때 쓰는 자료로서는 0점에 가까웠다.</li><li>프로젝트나, 코드를 작성할 때 comment 를 좀더 세세하게 작성하도록 노력해야겠다.</li></ul></li></ol></li><li>지적해주신 comment<ol><li>Regression 에서 intercept 의 의미<ul><li>실제로 모델링한 결과를 현실 데이터에서 사용하기 위해서는 intercept 를 꼭 추가해야한다고 말씀해주셨다.</li><li>comment 를 듣자 마자, 조금 찾아봤을 때, intercept 가 error 의 mean 값을 잡아준다고 한다.</li><li>이 부분은 좀더 보충이 필요하다.</li><li>더미변수를 사용하지만, Intercept 의 효과에 대해서.. 단지 해석의 의미로만 상수항을 생각하였는데, 좀더 본질적인 이유가 있는 것 같다.</li><li>꼭 보충할 것!</li></ul></li><li>데이터를 분석하는 과정에서 insight를 얻었을 때, 이를 꼭 알기 쉽게 기록하라.<ul><li>개인적으로 느꼈던 후기와 생각에서와 비슷한 취지의 말씀이었다. 자신과 다른사람이 알 수 있게 insight 를 꼭 기록하라고 말씀하셨다.</li></ul></li><li>R square 를 기준으로 분석을 진행할 때는 조심하여야 한다.</li></ol></li></ol></li></ul><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;181216-TWIL&quot;&gt;&lt;a href=&quot;#181216-TWIL&quot; class=&quot;headerlink&quot; title=&quot;181216 TWIL&quot;&gt;&lt;/a&gt;181216 TWIL&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;오늘-한-일은-무엇인가&quot;&gt;&lt;a href=&quot;#
      
    
    </summary>
    
      <category term="Diary" scheme="https://emjayahn.github.io/categories/Diary/"/>
    
    
  </entry>
  
  <entry>
    <title>181215-TodayWhatILearned</title>
    <link href="https://emjayahn.github.io/2018/12/15/181215-TodayWhatILearned/"/>
    <id>https://emjayahn.github.io/2018/12/15/181215-TodayWhatILearned/</id>
    <published>2018-12-15T14:32:26.000Z</published>
    <updated>2018-12-16T13:08:01.140Z</updated>
    
    <content type="html"><![CDATA[<h1 id="181210-TWIL"><a href="#181210-TWIL" class="headerlink" title="181210 TWIL"></a>181210 TWIL</h1><hr><h2 id="To-Do-list"><a href="#To-Do-list" class="headerlink" title="To-Do-list"></a>To-Do-list</h2><ol><li>DataThon 발표 준비</li><li>Pandas 4장 공부</li><li>알고리즘 문제풀기</li><li>(Study) 프로젝트 미니(웹어플리케이션) 준비</li><li>(Project) Classification Project Data EDA</li></ol><h2 id="오늘-한-일은-무엇인가"><a href="#오늘-한-일은-무엇인가" class="headerlink" title="오늘 한 일은 무엇인가"></a>오늘 한 일은 무엇인가</h2><ol><li>Datathon 발표준비</li><li>알고리즘 문제 풀기</li><li>(Study) 프로젝트 미니(웹어플리케이션) 준비</li></ol><hr><h2 id="내일-할-일은-무엇인가"><a href="#내일-할-일은-무엇인가" class="headerlink" title="내일 할 일은 무엇인가"></a>내일 할 일은 무엇인가</h2><ol><li>Pandas 4장 공부</li><li>Support Vector Machine 공부</li><li>알고리즘 문제풀기</li><li>NoSQL, MySQL syntax 정리<br><br></li></ol><hr><h2 id="무엇을-느꼈는가"><a href="#무엇을-느꼈는가" class="headerlink" title="무엇을 느꼈는가"></a>무엇을 느꼈는가</h2><ul><li>데이터톤 발표를 준비하면서, 제출했던 코드와 과정을 다시 살펴보니 Markdown 이나<br>주석이 부족함을 느꼈다. 다시 볼 때 좀더 편할 수 있도록, 코드와 과정을 다시 이어 나가는데 시간을 덜 소비하도록 나름 신경써서 작성하며 진행했는데, 다시 보려고 하니 머릿속에 있었던 것들이 다 작성되어 있지 않았다. 지금은 데이터톤에서 얼마 지나지 않았기 때문에, 기억에 남는 것이겠다. 하지만 추후에 다시 볼때는 기억이 나지 않아, 내가 작성한 코드와 문서임에도 불구하고 그 맥락을 이해하기 위해 처음부터 읽어 볼 것이다.</li><li>앞으로는 좀더 주석과 마크다운 문서에 신경을 많이 써야겠다. 짤막하게라도, 데이터 분석과정 중에 들었던 생각들을 작성해 놓아야, 그 시간이 지난뒤에 Develop 을 하던, 복기를 하던 계속 생각의 흐름을 이어 나갈 수 있을 것이다.</li><li><code>일일코딩</code>, <code>DailyCommit</code> 등에 관한 글을 읽었다. 개인의 다짐과도 비슷하고, 개인 프로젝트로 개발자들이 많이 하는 것 같다. 글을 읽은 직후에는 나도 하고 싶다는 생각을 했지만, 과연 할 수 있을 것인가 하며 반문을 하였다. 다짐의 문제라고 하기엔, 너무 정신 없는 나날을 보내고 있기에.. 도전할 것인지 하루만 더 고민해봐야겠다.</li></ul><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;181210-TWIL&quot;&gt;&lt;a href=&quot;#181210-TWIL&quot; class=&quot;headerlink&quot; title=&quot;181210 TWIL&quot;&gt;&lt;/a&gt;181210 TWIL&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;To-Do-list&quot;&gt;&lt;a href=&quot;#To
      
    
    </summary>
    
      <category term="Diary" scheme="https://emjayahn.github.io/categories/Diary/"/>
    
    
  </entry>
  
</feed>
